{"version":3,"mappings":";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;cAQA;AACA;;;AACA,+BAEA;AACA;AACA;;AACA,wCACA;AACA;;AACA,wCACA;;AACA,yCACA;AACA;;AACA,qDAEA;AACA;;AACA,qCACA;;AACA,uCACA;;AACA;;AAEA;AACA;AACA;;AACA;AACAA;AACA;;AACAC,mBAAa,6DAAYC,qDAAZ,EAAqBF,OAArB,EAAqBG,QAArB,CAAb;AACA,SANA;AAOA,QAEA;AACA;AACA;AACA;AACA;AACA;AACA;;;AACA;AACA;AACAC,gBAAU,iEAAiBC,QAAjB,CADV;AAEAC,mCAFA;AAGAC,6CAHA;AAIAC,2BAJA;AAIA;AACAC;AALA;AAOA;;AAEA;AACA;AACA;AACA;;AACA,uBAAiB,6DAAaC,iBAAb,CAAjB;AACAL;AACAA;AACAA;AACA;AACA,QAEA;AACA;;;AACA;AACA;AACA;AACA;;AACA;;AACAM;AACAA;AACA;AACA;AACA,QAEA;AACA;AACA;;;AACA;AACA;AACA;AACAV,qBAAe,4DAAI,IAAJ,EAAI;AAAQW;AAAR,aAAJ,CAAf;AACA,WAFA,MAEM;AAAqC;AAC3CX;AACA,WAFM,MAEA;AAAO;AACbA,qBAAe,kFAA0BY,IAA1B,EAA0BD,IAA1B,CAAf;AACA;AACA,SARA,MAQI;AAAO;AACX;AACAX;AACA,WAFA,MAEM;AAAqC;AACrC,sFAAkBY,IAAlB,EAAkB;AACxBZ,uBAAiB,4DAAIa,MAAJ,CAAjB;AACO,aAFD;AAGN,WAJM,MAIA;AAAO;AACbb;AACA;AACA;AACA;;AAEA;AACA;;AACA;AACA;AACA;;AACA;;AAEA;AACA;AACAc;AACA;AACA;;AAEA;AACA;AACA;AACA;;AACAC;AACAC;AACAC;AACA,WAHA;AAIA;;AAEAC;AACA;AACAC;AACA,WAFA,MAEM;AACNT;AACAO;AACA;AACG,SAPH;AAQA,QAEA;AACA;AACA;AACA;;;AACA;AACA;AACA;AACA;AACA;AACA;;AACA;AAAmC;AACnC;AACA;;AACA;AACA;AACA;AACAG;AACAC,8CAAwC,uDAC1B,qDAAIL,MAAJ,EAAI,0BAAJ,CAD0B,EAExC;AAAeb;AAAf,mBAFwC,CAAxC;AAIAmB;AACW,iBANX;AAOS,eART;AASO,aAhBP;AAiBA;AACG,SArBH;AAsBA;;AAEA;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACAC;;AACA;AAAkB;AAClBC;AACA;AACA;;AAEA;AACA;AACA;AACA;;AACAC;AACA,mEACAC,kBACAC,aADA,EACAA,mBADA,EACA,KADA,EACA,KADA,CADA;;AAGAC;AACA;;AACA;AACA;AACAC;AACA;AACA,aANA;AAOK,WAXL;AAYA;;AAEAC;AACA;AACA;;AACAC;AACA;;AACA;AACA;AACA;;AACAC;AAEA,qDACAC,UADA,CACAP,qBADA;;AAGAQ;AACA;;AACA;AACA;AACAT;AACAU;AACAD;AACA,eALA,MAKU;AAAO;AACjBjB;AACA;AACA,aAVA;AAWA,WArBA;AAsBG,SAzBH;AA0BA;;AAEA;AACA;AACA;AACAmB;AADA;AAGA,SAJA,CAII;AACJ;AACAC;AADA;AAGA;AACA;;AAEA,iCAA2BC,yDAA3B;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,+CAAyCC,OAAzC,EAAkDA,GAAlD,EAAkD;AAClD;;AACA,yBAAmB,kEAAS7B,OAAT,CAAnB,EAA4B;AAC5B;AACA;;AACAA,8BAAwB,iEAAQA,GAAR,EAAQ8B,cAAR,EAAQC,MAAR,CAAxB;;AACA;AACAC;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,8BAAwBC,oDAAxB;AACA;AACA;AAEE,sFAAqBC,QAArB,EAAqBC,QAArB,EAAqB;AACvB;AACA;AACA;;AACAC;AACG,SALD;;AAOF;AAEA,wBACAC,SADA,EACAC,YADA,EAEAC,YAFA,EAGAC,WAHA,EAGAC,oBAHA,EAIAC,UAJA;AAMA;;AACA;AACA;AACA;;AACAhB;AACAA;AACAA;AACAA;AACAiB;AACAC;AACAC;AACAC;AACAC;;AAEAA;AACAC;AACAC;AACA,WAHA;;AAKAC;AACA;AACAC;AACA;AACA;;AACAC;AACK,WANL;AAOA;;AAEA;AACAC;AACAJ;AACA;;AAEA;AACI,8EAAWlB,iBAAX,EAAWG,QAAX,EAAWoB,GAAX,EAAWC,WAAX,EACJ7B,GADI,EACJ8B,OADI,EACJC,QADI,EACJ3B,IADI,EACJ4B,kBADI;AAEJ;;AAEA;AACA;AACA;AACA,WAHA,CAIA;AACA;;;AACAV;AACAD;AACA;;AAEA;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACAY;AACA;AACA;;AAEA;AACA;;AAEA;AACAJ;AACA;;AACAhD;AACA;;AAEA,iDAA2CsB,OAA3C,EAAoDA,GAApD,EAAoD;AACpD;;AACA,+BAAyB,kEAAS+B,WAAT,CAAzB,EAAkC;AAClCrD,0BADkC,CACb;;AACrB;AACA;;AACA;AACAF;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEAwD;AACAvE;AACA;;AAEA;AAEA;;AACAe;AACA;AACA,wBAAkB,6DAAYd,wDAAZ,EAClB,yCACA0B,MAFkB,CAAlB;AAGA6C;AACAxE;AACA,aANA,MAMQ;AACRA;AACA;AACA,WAVA;AAWA;;AAEA;AAGA;AACA4C;AACA;AACA6B;AACA;;AACA;AACAC;AACA;AACS,eALT;AAMA;AACK,WATL;;AAUA;AACA;AACA;;AACA;AACA;;AAEA;AACA;AACAC;AACA;AACA;;AACAD;AACAE;AACA;AACAJ;AACA;;AACAvD;AACO,aALP;AAMK,WAPL;AAQA;;AAEA,wFACA4D,QADA,EACAC,KADA,EACAC,UADA,EACA/E,QADA,EACA;AAEAsE;AACAA;AAEA;AACA5D;AACAA;;AAEA;AACAA;AACA;;AAEA,mDACA+D,oCADA;;AAEA;AACA,iFACAI,QADA,EACAE,UADA,EACA/E,QADA;AAEA;;AAEAgF;AACArB;AAEAsB,iEACAJ,QADA,EACAE,UADA,EACA/E,QADA;AAEA;;AAEA,wEACA6E,QADA,EACAE,UADA,EACA/E,QADA,EACA;AAEA;AACA;AAEAU;AACA;AACA;;AAEA;AACA;;AAEA;AACAwE,iDAA2C,4DAAWZ,gBAAX,CAA3C;AACA;;AAEA;AACAa;AACA;;AAEA/E,2CAXA,CAYA;AACA;;AACA,0EACAgF,mBADA;AAEA;AACAC;AACA;;AAEA;AACA;AACAC,+BAFA,CAE0B;;AAC1BA,gCAHA,CAG2B;;AAC3B;AACA;;AACAC;AACA;AACAC;AACA,aAHA;AAIA;;AAEA;AACAtB;AACAuB,sBADA;AAEAjF,6BAFA;AAGAkF;AAHA;AAKAzB;AACA0B;AACA;;AAEA;AAEAH;AACAA;AACA;;AAEA,+EACAX,QADA,EACAE,UADA,EACA/E,QADA,EACA;AAGA;AAEA;AACA;;AAEA;AACA;AACAiF,qEACAJ,QADA,EACAE,UADA,EACA/E,QADA;AAEA;AACA;;AAEA;AACA4F;AACAC;AACA;;AAEA3E;AACA;;AACA;AACA;AACA;AACA4E;AACA;AACAC;AACA,aANA,MAMQ;AACRH;AACAC;AACA;AACK,WAZL;AAaA,SAtTA,CAwTA;AACA;;;AACA;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA7F;AACA;AACA;;AAEA;AACA;AACA;AACAO,sBADA;AAEAyF;AAFA;AAKAjF;;AACAA;AACA;AACA;AACA;AACAuE,iCAJA,CAI4B;;AAC5BA,kCALA,CAK6B;;AAC7BrE;AACA,aAPA;AAQA;;AACA,0BAAoBsB,oBAApB,EAA0CA,GAA1C,EAA0C;AAC1C0D,8BAD0C,CACjB;AACzB;AACA;;AAEA;AAGA;;AACAV;AACA;;AACA;AACA,gCADA,CAC2B;AAC3B;;AACA;AACA5D,4BADA;AAEAf;AAFA;AAIA;AACA4E;AACA,WAXA;AAYA;AACA,QAEA;AACA;AACA;AACA;;;AACA;AAEA;AACAU;AACA,SAJA,CAMA;AACA;AACA;AACA;;;AAEA,oEACA,4CADA,IAEAA,aAFA,IAEA,WAFA;AAIA;AACA;AACA;;AAEA;AACAC;;AACA;AACAC;AACA;AACA;;AAEA;AACAC;;AACA;AACAD;AACA;AACA;;AAEA;AACA;AAA6B;AAC7B;AACA,WAHA,CAIA;;;AACA;AACA;;AACA;AACA;AACAE,uEACA,IADA,EACAC,kBADA;AAEA,aAHA,CAGQ;AACR;AACA,iCADA,CAC4B;AAC5B;AACA;AACA,WATA,MASM;AACND;AACA;;AACAC;AACAF;AACAF;AACAK;AACAA;AACA;;AAEA;AACA;;AACA;AAAmB;AACnB;AACA,WAJA,CAKA;;;AACAJ;AACA;;AAEA;AACAK;AAAoB;AAApB;AACAD;AACAA;AACA,SAJA,MAII;AACJA;AACA,SAFI,MAEA;AACJA;AACA;AACA,QAEA;;;AACA;AACA;AACA;AACAA;AACA;AACA,SALA,CAMA;;;AACA;;AAEA;AACA;;AACA;AACAE;AACAxE;AACA,WAHA,MAGM;AACNyE;AACAC;AACAC;AADA;AADA;AAKA;AACA;;AAEAL;AACA;;AAEA;AACA;AACA;AACA;AACAM;AACAzD;AACA;AACA8C;AACA,aAFA,MAEQ;AACRA;AAA8BY,wBAA9B;AAA8B1E;AAA9B;AACA;;AACAd;;AACA;AACA6E;AACA;AACA,WAVA;AAWG,SAZH;AAaA;;AAEA;AACA;AACA;AACA;AACA;AACA,aAFA,MAEQ;AACR;AACA;AACA,WANA,MAMM;AACN;AACA;AACA,aAFA,MAEQ;AACR;AACA;AACA,WANM,MAMA;AACN;AACA;AACA,aAFA,MAEQ;AACR;AACA;AACA,WANM,MAMA;AACN;AACA;AACA,SAtBA,CAsBI;AACJ;AAAY/D;AAAZ;AACA;;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;;AACA;AACAkE;AACAS;;AACA,+BACA,iEADA,EACA;AACA;AACA;AACA,4BAAsB,6DAAY/G,qDAAZ,EACtB+G,kBADsB,EACtBA,qBADsB,CAAtB;AAEA;AACA;;AAEA;;AAEA;AACAC;AACA;;AACA;;AACA;AACA;AACA;;AACA;AACA7E;AACAA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEAqB;AACAyD;AACA,SAFA;AAIA;;;AACA;AACAC;AACA;AACAC;AACA;AACK,WAJL;AAKA;;AAEA;AACA;AACA;AACA;;AACA;AACAC;AACA;;AACA;AACAT;AACAC;AADA;AADA;AAKA;;AACAL;AACA,SAtEA,CAwEA;AACA;;;AACA;AACA;;AACAc;AACAjG;;AACA;AACA,8BAAwB,iEAAgBjB,QAAhB,CAAxB;;AACA;AACAiB;AACA;AACA;;AACAkG;AACA,WATA;AAUA;;AAEA;AACA;AACA/G,2BADA;AAEAuG,4BAFA;AAGAS;AACA9B;AADA;AAHA;AAOA;;AACA;AACA;AACAxB,gCADA,CAEA;;AACA7C;AACAA;AACA;AACA,WAPA,MAOM;AACN6C;;AACA;AACAuD;AACA;AACA;AACA;;AAEA;AACA,oDAA8ClF,OAA9C,EAAuDA,GAAvD,EAAuD;AACvD;AACA;AACA;;AACA;;AACA;AACA;AACA2B;AACA;AACA;;AACA;AACA;AACAwD;AACA;AACA;;AAEA;AACA;AACA;AACA;;AACAC;;AACA;AACAzF;AACA;AACA;;AAEA;AACA;;AACA;AACAwE;AACA;;AACAiB;AACA;;AAEA;AACA;AACAC,gCADA;AAEAC,6BAFA;AAGAC;AAHA;AAMA;;AACA;AACAC;AACA;;AACA/H;AACA;;AAEA;AACA;AACAgI;AACA,WAFA,MAEM;AACNC;AACA;AACA,SAvKA,CAyKA;;;AACA;AACA;AACA;;AACA;AACA;AACA;;AACA;AAAsB;AACtB;AACA,SAlLA,CAmLA;AACA;;;AACAC;AACA,QAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;AACA;AACA;AACA,wBAAkB,4DAAI,IAAJ,CAAlB;AACA;;AAEAnH;AACA;AACA,kEAFA,CAGA;AACA;;AACAO,qDACA6G,oCADA;AAEA,WAPA;;AASApH;AACA;AACA;AACAuE;AACAA;AACAhE;AACA,WANA;AAOG,SApBH,WAoBG;AACH,uBADG,CACe;AACf,SAtBH;AAuBA;;AAEA;AACA;;AACAS;AACAjB;AACA,SAFA;AAGA,QAEA;;;AAEA;AACA;;AAEA;AACA;AACAsH;AACA,SAFA,CAEI;AACJ;AACA;AACA;AACAC;AACA;AACA;;AAEA;AACA;AACA;AACA;;AACAC;AACAC;AACA;;AAEA;AACAA;AACAC;AACAC;AACAH;AACM,qEAAQ;AACdI;AACO,aAFD;AAGD,WANL;AAOG,SARH;AASAA;AACA;;AAEA;AACAlG,eAAS,sDAAKA,IAAL,CAAT;;AAEA;AACA,kCAA4B,sDAA5B;AACA+B;AACAA;AACA;AACAoE;AACApE;AACA;AAHA;AAKA;;AAEA,yCAAmC5B,oDAAnC,CAAsCH,YAAtC;AAEAA;AACA;AAEA;;AACA;AACAoG,oBADA,CACe;AACf;;AAEA;AACA;AACA,qBAAe,6DAAYpG,IAAZ,CAAf;AACA,mCAA6BG,oDAA7B;AAEA;AACA;AACA;AACA;;AAEA;AACA;AAAwC;AACxC;AACA;;AAEA;AACA;;AAEA;AACA;AACAkG;AAEA;;AACA;AAA0C;AAC1C;AACA;;AAEA;AACA;AACA;;AACAC;;AACA;AACA5E;AACA,aAfA,CAgBA;AACA;;;AACA;AACA;AACAqD;AACAS;AACA1G;AACa,mBAFb;AAGW,iBAJX;AAKS,eANT;AAOA,aARA,MAQQ;AACR;AACA;AACA;;AAEA;AACA;;AACA,sDAAgDiB,OAAhD,EAAyDA,GAAzD,EAAyD;AACzD;AACA;AACA;;AACA;;AACA;AACA;AACA;;AACA;AACAwG;AACA;;AAEAC;AACA,oDAA8CzG,OAA9C,EAAuDA,GAAvD,EAAuD;AACvD;AACAC;AACA;AACA;AACO,aANP,WAMOA,aANP;;AAQA;AACAN;AACA;AACA,WAhEA,CAkEA;AACA;AACA;;;AACA;AACA+G;AACA;AACA;AACAC;AACAC;AACAC;;AACA;AACAC;AACA;AACO,aANP;AAOK,WAVL;AAWA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,WATA,CAWA;;;AACA;AACA;;AACAtI;AACAD;AACA,WAFA;AAGA;;AAEA;AACA;AACA;AACA;;AAEA;;AACA;AAAoB;AACpB;AACA,WARA,CASA;;;AACAuC;AACAjD;AACAkJ;AACAC;AACA,WAJA;AAKA;;AAEA;AACA/G;AACA0B,4BADA;AAEAsF;AAFA;AAIA;;AAEA;AACA;AACA;AACA;AACAxB;AACA,WAJA,MAIM;AACNrD;AACA;AACA;;AAEA;;AACA;AACA8E;AACA;;AACA;;AACA;AACA;AACA;;AACArH;AACAA;AACAA;AAEAkB;AACAD;AACAiE;AAEA,wDACA5F,wCADA,GACA,IADA;AAGAwG;AACA;;AAEA,0BAAoBvF,oDAApB;AACA;AACA,4BAAsBA,oDAAtB;;AAEA;AACA;AAEA+G;AACAC;AACG,SAFH,EAEG3J,QAFH,EAEGgE,eAFH;AAGA;;AAEA;AAEA;AAEA;AACAA,yBALA,CAOA;;AACA;AACA;AAAoD4F;AAApD;AACAC;AAAwCC;AAAxC,aACAC,WADA,CACA,aADA,EACA,aADA,EACA;AAAkDC;AAAlD,WADA;AAEAH;AAAwCD;AAAxC;AACAC;AAAsCD,yBAAtC;AAAsCE;AAAtC;AACAD,0DANA,CAQA;;AACAxG;AAA8D2G;AAA9D,aATA,CAWA;;AACAH;AAAuCD;AAAvC,aAZA,CAcA;;AACA,0EACA;AAAOE;AAAP,WADA;AAEA3H;AACAA;AAA0D6H;AAA1D;AACA,SA3BA,CA6BA;AACA;AACA;;;AACA;AACA;AACA3G;AAA8D2G;AAA9D;;AAEA3G;AACA;;AACA;AACA;AACA,4BAAsB,0DAASjD,QAAT,CAAtB;AACAA;AACAiD;AACAnB;AACA,aANA,MAMQ;AACRlC;AACA;AACA,WAXA;AAYA,SAhDA,CAkDA;;;AACA;AACA6J;AAAuCD;AAAvC,aACAG,WADA,CACA,aADA,EACA,aADA,EACA;AAAkDC;AAAlD,WADA;AAEA,SAtDA,CAwDA;;;AACA;AACA;AACA;AACA;AAEA;;AACA9H;AACA;;AACA;AACA;AACA;AACA,0BAAoB,0DAAW+H,KAAX,CAApB;AACA,wBAAkB,2DAAU7J,QAAV,CAAlB;;AACA;AACA,kDADA,CAEA;AACA;;AACA;AACA;AACA;AACA;AACA;;AACA8J;AACAA;;AACA;AACA;AACA7G;AACAnB;AACA,mBAJA,MAIc;AACd;;AACA;AACAiI;AACA;;AACAnI;AACAkI;AACA;AACA,iBAdA;AAeA,eAxBA,MAwBU;AACVhI;AACA;AACA,aAhCA,MAgCQ;AACRpB;AACA;AACA,WArCA;AAsCA,SArGA,CAuGA;;;AACA;AACA,0EACA;AAAOgJ;AAAP,WADA;AAEA3H;AACAA;AAA0D6H;AAA1D;AACA,SA7GA,CA+GA;;;AACA;AACA;AACA;AACA,qEAHA,CAKA;AACA;AACA;;AACA;;AACAjJ;AACA;;AACA;AACA,gCADA,CAC2B;AAC3B;;AAEAiB;AACA;;AACA;AACA,kCADA,CAC6B;AAC7B;;AACA;AACA;AACA;AACA;;AACA,8BAAwBoI,eAAxB,EAAyCA,GAAzC,EAAyC;AACzC;AACAC,6CAFyC,CAED;AACxC;;AACA;;AACA,0BAAoBD,kBAApB,EAAwCA,GAAxC,EAAwC;AACxC;AACAjI;AACA5B,0BADA;AAEAyF;AAFA;AAIA;;AACA9D;AACA,aAtBA;AAuBA,WA7BA;AA8BA,SAvJA,CAyJA;AACA;AACA;AACA;AACA;AACA;;;AACA;AAEA;AACA;AACA;AACAzB;AACA;AACA;;AACA;AACA,WATA,CAWA;AACA;;;AACA;AACA;AACA;;AACAyB;AACA;;AACA;AACA,qBADA,CACgB;AAChB;;AACA;AAEA9B,yDACQ,2DAAUA,QAAV,CADR;;AAGA;AACA;AACA;AACA;AACA;AACA,mEACAsB,6BADA;AAGA;;AACAX;AACA;;AACA;AACAX;AACA;AACA;;AACA;;AACA;AACAkK;AACA;;AACApI;AACA,eAXA;AAYA;;AAEA;AACA,6DACA9B,mBADA,EACAA,gBADA;AAGA;;AACAW;AACAmB;AACA,eAFA;AAGA;;AAEA;AACA;AACA;;AAEAqI;AACA,WAhDA;AAkDA;;AAEAvG;;AACAA;AACA;AACA,SAFA;;AAIAA,kBAAY,0DAAS;AACrBhE;AACG,SAFS,CAAZ;;AAIAgE;AACAwG;AACA,SAFA,CA5OA,CAgPA;AACA;;;AACAxG;AACA;AACA;AACA;AACA;;AACA;AACA,uDACA,uCADA,EACA,UADA;;AAEA;AACA;AACA;;AACA5B;AACA;;AAEA;AACApC;AAAqBU,sBAArB;AAAqBN,gCAArB;AAAqBqK;AAArB;AACA;;AAEArI;AACAhC,uDADA,CAEA;AACA;AACA;AACA;AACA;;AACA;AACAoE,oBAAc,6DAAYvE,uDAAZ,EAAuB,SAAvB,CAAd;AACA;AACA;;AAEA;;AACA;AACAyF;AACA,4BAAsB,0DAAStF,QAAT,CAAtB;;AACA;AACAoE,sBAAgB,6DAAYvE,uDAAZ,EAAuB,SAAvB,CAAhB;AACA;AACA;AACA,aAPA,MAOQ;AACRyF,kCAA4B,uDAAMlD,QAAN,EAAMpC,QAAN,CAA5B,GAAkCoC,QAAlC;AACA;;AAEA;AACA;;AAEAgE;AACA9F;;AACA;AACAA;AACA;;AACA;AACA8D,sBAAgB,6DAAYvE,uDAAZ,EAAuB,SAAvB,CAAhB;AACA;AACA;;AACA0E;AACA,aAVA;AAWA,WAtCA;AAuCA,SAzDA;;AA2DAX;AACA;;AACA;AACA5B;AACA,WAFA,MAEM;AACN,uDACA,uCADA,EACA,UADA;;AAEA;AACA;AACA;;AACAA;AACA;;AACA;AACA;;AAEAA;AACA;AACAhB;AACApB;AACO,aAFP;AAGA,WALA;AAMA,SArBA;;AAuBAgE;AACA;AACA;AAEA;;AACA;AACA;AACA;;AACA;;AACA5B;AACA8E;AACA,WAFA;;AAGA9E;AACA;AACAgF;AACA,WAHA;;AAKAhF;AACApC;AACA0K,iCADA;AAEAC,mCAFA;AAGA;AACAC;AAJA;AAMA,WAPA;AAQA,SAzBA;;AA2BA5G;AACA6G;AACA,SAFA;;AAIA7G;AACA;AACA,SAFA;;AAIAA;AACA;AACA;AACA8G;AACAC;AACA/K;AACA,SANA;;AAQAgE;AACA;;AACA;AACA;AACA;;AACA;AACA;;AACAjD;AACA;;AACA;AACAf,uBAAiB,6DAAYC,uDAAZ,CAAjB;AACA,aAFA,MAEQ;AACRD;AACA;AACA,WAPA;AAQA,SAfA,CA/WA,CAgYA;AACA;AACA;;;AACAgE;AACA,wBACAjB,SADA,EAEAC,YAFA,EAGAC,YAHA,EAIAE,oBAJA;AAMA;;AACA;AACA;AACA;;AACA;AAEA;;AAEAE;AACA;AACM,4EAAejD,iBAAf,EAAe,uBACrB4K,OADqB,EACrBP,GADqB,EACrBjI,IADqB,EACrB;AACA;;AACA;AACAA;AACA;AACO,aAND;AAON2C;AACA;AACA;AACA/C,2CACA6I,gDADA;AAEA,WAdA;;AAeA7I;;AACAA;AACApC;AACA,WAFA;AAGA,SAlCA;;AAqCAgE;AACA;;AACA;AACA;AACA;;AACA;AACA;AAEAjD;;AACAA;AACA;;AACA;AACAf,uBAAiB,6DAAYC,uDAAZ,CAAjB;AACA,aAFA,MAEQ;AACR,wCADQ,CAC2B;;AACnCD;AACA;AACA,WARA;AASA,SAlBA;;AAoBAgE;AACA;AACAhE;AACAwC;AACA;;AACA,gCALA,CAK2B;;AAC3B;AACA;;AACA;AACA9B;AACA,WAFA,MAEM;AACNA;AACA;;AAEA;AACA;;AACA;AACA;;AACA;AACA;AACA;;AACAwK;AACAA;;AACAA;AACA;AACAlL;AACA;AACA,aAJA;AAKA;;AAEA;AACA;;AACA;AACAe;;AACAA;AACA;;AACA;AACAf,yBAAmB,6DAAYC,wDAAZ,CAAnB;AACA,eAFA,MAEU;AAAO;AACjB;;AACAc;AACAoK;AAAmB1F,4BAAnB;AAAmBjF,+BAAnB;AAAmBkF;AAAnB;;AACA;AAA4B;AAC5B1F;AACA;AACA,iBALA;AAMA;AACA,aAbA;AAcA,WAhBA,MAgBM;AAAO;AACbe;;AACAA;AACA;AACAf,uBAAiB,6DAAYC,wDAAZ,CAAjB;AACAqF,iCAHA,CAG4B;;AAC5BA,kCAJA,CAI6B;AAC7B,aALA;;AAMAvE;AACAoK;AAAe1F,wBAAf;AAAejF,2BAAf;AAAekF;AAAf;;AACA;AAAwB;AACxB1F;AACA;AACA,aALA;AAMA;AACA,SA/DA;;AAiEAgE;AACA;AACAhE;AACAwC;AACA;;AACA;;AACA;AACA;;AACA;AACA;AACA;;AACA0I;;AACAA;AACA;AACAlL;AACA;AACA,aAJA;AAKA;;AACA;AACA;AACA;AACA;AAEAe;;AACAA;AACA;;AACA;AACAf,uBAAiB,6DAAYC,uDAAZ,CAAjB;AACA,aAFA,MAEQ;AACRmL;AACAD;AAAe1F,wBAAf;AAAejF,sBAAf;AAAekF;AAAf;;AACA;AAAwB;AACxB1F;AACA;AACA;AACA,WAXA;AAYA,SApCA;;AAsCAgE;AACAO,sDADA,CAGA;;AACA;;AACA;AACA8G;AACAN;AACA;;AACA;;AAEAhK;AACA;AACAuK;;AACA,gBAAU,qEAAeC,sBAAzB,EAAyB;AACzB;AACA;;AACAvL;AAAuB;AAAvB;AACA,WAPA;;AASAe;AACA,SArBA;;AAuBA;;AAEA;AACA+J;AACA9G;AACA,iBAAW,yDAAQ;AACnBhE;AACK,WAFM,CAAX;AAGA;;AAEA;AACAsL;;AAEAvK;AACA;;AACA;AACA,oCADA,CAC+B;AAC/B,WAJA,CAKA;;;AAEA,gDAPA,CAQA;AACA;;AAEA;AACAyK,uCADA,CACkC;AAClC;;AACA;AACAC,qCADA,CACgC;AAChC;;AAEA,4BACAC,sBADA,EACA;AACAC,2BAFA,EAEA;AACAC,4BAHA,EAGA;AACAC,yBAJA,CAIA;AAJA;AAOA;;AAEA;AACA;AACAtJ;;AACA;AACAuJ;AACA;AACA;;AAEAC;AACA,SApCA;;AAsCAhL;AAEA+J;;AAEAA;AACAA;AACAC;AACA,WAHA;;AAKAD;AACM,2EAAc,OAAd,EAAc,+BAAd,EAAcxF,cAAd;AACNwF;AACAC;AACA,WAJA,CATA,CAeA;AACA;AACA;AACA;AACA;AACA;;;AAEA,qCACA3H,UADA,EAEA4I,yBAFA,EAGAjJ,SAHA,GAIA,WAJA;AAMA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AACAiB;AACAiI,0BADA;AAEAC,oCAFA;AAGAC;AAHA;AAMApB;AACAD,sBADA;AAEAsB;AAFA;AAIApM;AACA;;AAEA;AACA;AACA;AACA;;AACA;;AACA;AACAkM;AACA,aAFA,MAEQ;AACRxI,kDAA4C,sDAA5C;AACA;;AACAA;AACAtB;AACA,WA/DA,CAiEA;AACA;AACA;;;AACAA;AACAsB;AAAqClD;AAArC;AACA6L;AACA,WAHA,CApEA,CAyEA;AACA;AACA;;;AACAC;AACApF;AACAmF;AACK,WAHL,EA5EA,CAiFA;AACA;AACA;;AACA;AACA;AACAE;AACA;;AAEAA;AACAJ;AACAK;AACK,WAHL,EAzFA,CA8FA;AACA;;AACApK;AACAqK;AACAD;AACA,WAHA;;AAIApK;AACA,SArGA;;AAuGArB;AACA;;AAEA;AACA2L;AACA,WAFA,MAEM;AACNA;AACA;;AAEI,yEAAc,OAAd,EAAcA,GAAd;AACJ1M,mBAAa,6DAAYC,qDAAZ,EAAqByM,GAArB,CAAb;AACA,SAXA;AAYA;;AAEAC;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,SAJA,CAII;AACJ;AACA;AACA,OAbA;;AAeA;AACAtE;AACA;AAEA;;;AAAAuE,uCAAe7K,KAAf;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;ACx8DA;AACA;AACA;AACA8F;AADA;AAGA;AACA,wBAAkB,uDAAM;AAAEd,oBAAF;AAAE8F;AAAF,WAAN,EAAgCrK,IAAhC,CAAlB;AACA;AACA;AACK,WAFL;AAGA;AACAwB;AACA;AACA;AACA;AACA;AACA;;;AACA;AACA8I;AACA;;AACAA;AACAxL;AAAgCyF,wBAAhC;AAAgC1E;AAAhC;AACO,aAXP;AAYK,WAbL;AAcG,SAnBH,GAmBG0K,IAnBH,CAmBG;AACHD;AACA;AACG,SAtBH;AAuBA;;AAEA;AACA;AACAE;AACA;AACG,SAHH,EAGG,EAHH;AAIA,QACA;;;AACA,oCACA,KADA,EAEA,MAFA,EAGA,cAHA,EAIA,UAJA,EAKA,YALA,EAMA,YANA,EAOA,YAPA,EAQA,oBARA,EASA,YATA,EAUA,WAVA,EAWA;AACA,uBAZA,EAaA,oBAbA,EAcA,yBAdA,EAeA,2BAfA,EAgBA,oBAhBA,EAiBA;AACA,gBAlBA,IAqBA;;AACA,gCACA,cADA,EAEA;AACA,uBAHA,EAIA,oBAJA,EAKA,yBALA,EAMA,2BANA,EAOA,oBAPA;;AAUA;AACA;AACA,iBAAW,6DAAY/M,uDAAZ,CAAX;AACA;;AACA;AACA;AACA;AACA;AACAgN,oCADA;AAEAzM;AAFA;AAIA;;AAEA;AACA;AAEA;AACA;;AAEA,kDAA4C+B,OAA5C,EAAqDA,GAArD,EAAqD;AACrD2K;AAA4BC;AAA5B,aAA8C,KAA9C;AACA;;AAEA;AACAC,kBADA;AAEAF;AAFA;AAIA,QAEA;AACA;;;AACA;AACA;AACAzK;AACA4K;AADA;AAGA;;AAEA;AACA;AACA;AACA;AAAcF;AAAd;;AACA;AACA3K;AACA;;AAEA;AACA;AACA9B,sBAAgB,sDAAhB;AACA;;AACA4M,qBAAe,oDAAG5M,GAAH,EAAG+B,yBAAH,CAAf;;AACA;AACA8K;;AACA;AACA;AACA;;AACA7M;AACA0M,iCADA;AAEAF;AAA2BC;AAA3B,iBAA6C,sBAA7C;AAFA;AAIAK;AACA,WAVA,MAUM;AACN9M;AACA0M,oBADA;AAEAF;AAFA;AAIAM;AACA;AACA,SAtBA,MAsBI;AACJ;AACA9M;AACA8M;AACAF;AACA;;AACA;AACAC;;AACA;AACA;AACA;;AACAC;AACAF;AACA5M;AACA0M,0BADA;AAEAF;AAFA;AAIA;AACA;;AAEE,uEAAcxM,OAAd;AAEFA;AAEA;AAAgBN,sBAAhB;AAA6BD;AAA7B;;AACA;AACA;AACA;AACA;;AACA;AACA,0BAAoB,6DAAYF,0DAAZ,EAA0B8G,GAA1B,CAApB;AACA1E,8BAAwBpC,qEAAsB,IAAtB,GAAsB8G,GAA9C;AACA;AACA,aAJA,MAIQ;AACRF;AACA,aAFQ,MAEA;AACRA;AACA;AACA;AACA;;AACA;AACA;;AAEA;AACA;AACA,iBAAW,4DAAI1G,IAAJ,CAAX;AACA,SAFA,CAEI;AACJ,oBAAc,6DAAYF,mDAAZ,EACd,yCADc,CAAd;AAEA;AAAYoC;AAAZ;AACA;AACA;;AAEA;AACA;;AACA;AACA;AACA;;AAEAyD;;AACA;AACAA,qBAAe,kFAA0B2H,QAA1B,EAA0B3H,gBAA1B,CAAf;AACA,SAFA,MAEI;AACJA,qBAAe,4DAAI2H,QAAJ,CAAf;AACA,SAFI,MAEA;AAAO;AACX3H;AACA;;AACE,gEAAS2H,QAAT,EAAS;AACX3H;AACA9F;AACG,SAHD;AAIF;;AAEA;AACE,gEAAS8F,QAAT,EAAS;AACXA,oCADW,CAEX;;AACAA;;AACA;AACM,8FAA0BA,QAA1B,EAA0B;AAChCA;AACA9F;AACO,aAHD;AAIN,WALA,MAKM;AACA,wFAAoB8F,QAApB,EAAoB;AAC1BA;AACA9F;AACO,aAHD;AAIN,WALM,MAKA;AACNA;AACA;AACG,SAjBD;AAkBF;;AAEA;AACA;AACA;AACA;;AACA;AAAsC;AACtC0N;AACA,SAFA,MAEI;AAAO;AACXC;AACA;AACA;;AAEA;AAEA;AACA;AACA;;AAEA;AACA;AAEA/K;AACA,wEACA6B,sCADA,GACA,EADA;AAEA;;AAEA;AACA;AACA;;AAEA;AACAmJ;AACAC;;AACA;AACAC;AACA;AACA;;AAEA;AACA;AACAC,mEACAlL,QADA,EACAmL,mBADA;AAEA;AACA;AACG,SAvBH;;AAyBA;AACAC;;AACA;AACA;AACAjO;AACA,aAFA,MAEQ;AACRA;AACA;AACA;AACA;AACA;;AAEA,4DACAuC,CADA,EACAzB,EADA,EACAqD,QADA,EACA+J,QADA,EACA;AAEA,YAAM,0DAASC,aAAT,EAAS7J,oBAAT,KAAS,SAAf,EAAe;AACfJ;AACA;AACA,SALA,CAOA;;;AACA,oDAA8C,2DAAUiK,IAAV,CAA9C;AACA,mEACI,0DAASA,IAAT,EAASC,kBAAT,CADJ;AAEA,iFACI,0DAAS9J,gBAAT,CADJ;AAEA;;AAEA;AACA;AACA+J;AACAA;AACA/J;AACA;;AAEA,qBAAe,sDAAK6J,aAAL,EAAK7J,4BAAL,EAAKgK,QAAL,CAAf;AAEA,sCACAC,mEACA,qDADA,IAEAA,kEAHA;;AAKA;AACA,oBAAc,6DAAYtO,wDAAZ,CAAd;AACAiE;AACA;AACA;;AAEA;AACAI;AACAA;AACA;;AACA;AACAA,kDADA,CAC6C;AAC7C,SAzCA,CA2CA;;;AACA,4BAAsB,2DAAUA,gBAAV,CAAtB;AACA,kCAA4B,0DAASA,gBAAT,EAASkK,aAAT,CAA5B,CA7CA,CA+CA;AACA;;AACA,oEACAD,gDADA;AAGA;;AACA;AACA;AACAE;AACA,SAHA,MAGI;AACJ;AACAA,4BAAsB,0DAASnK,gBAAT,EAASoK,MAAT,CAAtB;AACA;;AAEAvK,+EACA,IADA,EACAW,KADA,EACAvC,CADA,EACAzB,EADA;AAEA;;AAEA;AACA;AACA;;AAEA,+EACAqD,QADA,EACA3B,IADA,EACAmM,eADA,EACA;AAEA;AACAL;;AAEA;AACA;AACA,8BAAwB,2DAAUhK,gBAAV,CAAxB;AACA,wBAAkB,0DAASA,gBAAT,EAASkK,aAAT,CAAlB;;AACA;AACAtK,kCAA4B,6DAAYjE,uDAAZ,EAAuB,SAAvB,CAA5B;AACA;AACA,WAPA,CASA;;;AACA;;AAEA;AACA,sBAAgB,6DAAYA,wDAAZ,CAAhB;AACAiE;AACA;AACA;;AAEA;AAEAC,oEACAW,KADA,EACAC,UADA,EACA/E,QADA;AAEA;;AAEA;AACA,4BAAsB4O,oDAAtB;AAEA;AACA;;AAEA;AACA;AACAD;AACA;AACA;;AAEA/L;AAEA,gCAA0B,0DAASiM,cAAT,CAA1B,EAAmC;AACnC;AACA7K;AAA4ByG;AAA5B,eAAoC;AACpCvG;AACA4K;AACO,aAHP;AAIA;AACA;;AAEA;;AACA;AACAC,uBADA,CACkB;;AAClBC;AACA,WAHA,MAGM;AACNA;AACA;AACG,SAlBH,EAzCA,CA6DA;AACA;;AACAA;AACA;;AAEA;AACA;AACAC;AACA,aAFA,MAEQ;AACRH;AACA;AACA;;AACA;AACA;AACA;AACA;;AAEA;AACAI,6EACAnK,UADA,EACAoK,UADA,EACAhL,QADA,EACA+J,QADA;AAEA,aAHA,MAGQ;AACR;AACA,2BAAqB,sDAAK,EAAL,EAAKW,+BAAL,EAAKP,QAAL,CAArB;AACAO;AACAA;AACAO;AACA;AACA;;AACAH;AACG,SA3BH;AA4BA;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;ACzcA;AACA;AACA;AACA;AACA;AACA;AACA,SAFA,CAEI;AACJ;AACA,iBAAWI,4CAAcC,GAAd,CAAX;AACA;AACA;;AAEA;AACA;AACA;AACA,SAFA,CAEI;AACJ;AACA,iBAAWD,gDAAkBE,IAAlB,CAAX;AACA;AACA;;;;;;;ACnBA;AACA;AACA;AACA;AACA;AACAC,0BAAiB;AACjB;AACAjH;AAAcyE;AAAd;AAEA;AACA;;AACA;AACAA;AACAC;AACAwC;AACAC;;AACA;AACAA;AACA,WAFA,MAEM;AACNA;AACA,WAFM,MAEA;AACNA;AACA,WAFM,MAEA;AACNnH;AAAkBkH;AAAlB;;AACA,qCAA+BlN,MAA/B,EAAuCA,GAAvC,EAAuC;AACvCoN;AACApH;AAAoByE,2BAApB;AAAoBC;AAApB;AACA;;AACA1E;AAAkBkH;AAAlB;AACA,WAPM,MAOA;AAAO;AACb3I;;AACA;AACA;AACAA;AACA;AACA;;AACAyB;AAAkBkH;AAAlB;;AACA,sCAAgClN,MAAhC,EAAwCA,GAAxC,EAAwC;AACxCwE;AACAS;AACAoI;AACAA;AACArH;AAAoByE,0BAApB;AAAoBC;AAApB;AACA;;AACA1E;AAAkBkH;AAAlB;AACA;AACA;;AACA;AACA,OA3CA,EA6CA;AACA;AACA;;;AACA;AACA;;AACA;AACA;AACAI;AACAC;AACA;;AACA;AACA;;AACA;AACAC;AACA,SAFA,MAEI;AAAkD;AACtD;AACAA;AACA,SAHI,MAGA;AACJC,0BADI,CACiB;AACrB;AACA;;AAEAR,sBAAa;AACb;AACA,2BAFa,CAES;;AACtB;AACA;AACA;AACA;;AACA;AACAS;;AACA,wCACAA,sBADA,IAEA,qCAFA,EAEA;AACA;AACA;AACA,aAFA,MAEQ;AACRC;AACA;AACA;AACA;;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AACA;AACA3N,qBADA,CACgB;;AAChB2N;AACA;;AACA;AACA3N,qBADA,CACgB;;AAChB2N;AACA;;AACA;AACA3N,qBADA,CACgB;;AAChB2N;AACA;;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACAC;AACA5N;;AACA;AACA6N;;AACA;AACAD;AACA,iBAFA,MAEY;AACZ5N;AACA;AACA;AACA;;AACA2N;AACA;;AACA;AACAG;AACAC;AACAC;;AACA;AACAC;;AACA,qDACAD,+BADA,EACA;AACAF;AACAC;;AACA;AACAC;AACA,mBAFA,MAEc;AACdA;AACA;AACA,iBATA,MASY;AACZ;AACA;AACA;;AACAL;AACA;;AACA;AACAO;AAAyBV,2BAAzB;AAAyBhO;AAAzB;AACAiO;AACAH;AACA;;AACA;AACAa;AAAuBX,2BAAvB;AAAkChO;AAAlC;AACAiO;AACAH;AACA;;AACA;AACA,8BACA,sDADA;AA3EA;AA8EA;AACA,OAlGA;;;;","names":["message","callback","pouchdb_errors__WEBPACK_IMPORTED_MODULE_4__","evt","data","metadata","winningRev","deletedOrLocal","seq","id","storedObject","doc","type","body","binary","cb","req","attObj","checkDone","attachments","fetchAttachment","readBlobData","row","resolve","count","deleteOrphanedAttachments","possiblyOrphanedDigests","IDBKeyRange","digest","countReq","attStore","revs","index","seqStore","openCursor","cursor","attAndSeqStore","txn","error","pouchdb_utils__WEBPACK_IMPORTED_MODULE_6__","i","opts","dbOpts","docInfoError","pouchdb_collections__WEBPACK_IMPORTED_MODULE_5__","docInfos","blobType","startTransaction","DOC_STORE","BY_SEQ_STORE","ATTACH_STORE","LOCAL_STORE","ATTACH_AND_SEQ_STORE","META_STORE","docStore","bySeqStore","attachStore","attachAndSeqStore","metaStore","metaDoc","updateDocCountIfReady","verifyAttachments","preconditionErrored","fetchExistingDocs","allDocsProcessed","api","fetchedDocs","results","writeDoc","onAllDocsProcessed","idbProcessDocs","docInfo","changesHandler$1","err","Object","digests","finish","verifyAttachment","isUpdate","delta","resultsIdx","docCountDelta","finishDoc","revsToDelete","compactRevs","winningRevIsDeleted","metaDataReq","e","getKeyReq","putReq","ok","rev","insertAttachmentMappings","numDone","collectResults","att","saveAttachment","digestSeq","add","batchSize","valuesBatch","onBatch","keysBatch","newKeyRange","keyRange","objectStore","pseudoCursor","values","onSuccess","target","result","keys","key","keyRangeError","stores","docCount","getMaxUpdateSeq","updateSeq","maxKey","docIdRevIndex","fetchAttachmentsIfNecessary","value","fetchDocAsynchronously","allDocsInner","processBatch","total_rows","offset","rows","returnVal","postProcessAttachments","onResultsReady","runBatchedCursor","parseInt","fun","PouchDB","running","queue","action","tryCode","applyNext","cancel","limit","lastSeq","numResults","promises","Promise","batchValues","fetchWinningDocAndMetadata","metadatas","winningDocs","onBatchDone","docIdsToMetadata","onGetMetadata","last_seq","objectStores","enqueueTask","init","keyPath","db","autoIncrement","createIndex","unique","docId","seqCursor","localStore","j","digestMap","metadataSeq","fetchMetadataSeq","idbBulkDocs","ctx","doc_count","update_seq","idb_attachment_format","idbAllDocs","idb","cachedDBs","revHash","encodeMetadata","tx","ret","oStore","openReq","openReqList","dbName","createLocalStoreSchema","addAttachAndSeqStore","addDeletedOrLocalIndex","migrateLocalStore","migrateAttsAndSeqs","migrateMetadata","migration","next","DETECT_BLOB_SUPPORT_STORE","name","instanceId","blobSupport","global","storeMetaDocIfReady","countDocs","blobSupportPromise","completeSetup","storedMetaDoc","msg","IdbPouch","__webpack_exports__","deleted","finalResults","then","obj","prefix","ids","status","pos","deterministic_revs","newRevId","revInfo","nRevNum","asBinary","preprocessString","preprocessBlob","overallErr","recv","done","preprocessAttachment","processedAttachment","docv","newEdits","prev","previousWinningRev","newDoc","revLimit","previouslyDeleted","winningRev$$1","newRevIsDeleted","newRev","overallCallback","pouchdb_collections__WEBPACK_IMPORTED_MODULE_3__","currentDoc","checkAllDocsDone","docsToDo","idsToDocs","nextDoc","updateDoc","docWritten","insertDoc","vuvuzela__WEBPACK_IMPORTED_MODULE_0__","str","json","exports","val","res","arrayPrefix","objPrefix","metaStack","lastMetaElement","element","stack","collationIndex","pop","parsedNum","numChar","parsedString","lastCh","numConsecutiveSlashes","ch","arrayElement","objElement"],"sources":["webpack:///node_modules/pouchdb-adapter-idb/lib/index.es.js","webpack:///node_modules/pouchdb-adapter-utils/lib/index.es.js","webpack:///node_modules/pouchdb-json/lib/index.es.js","webpack:///node_modules/vuvuzela/index.js"],"sourcesContent":["import { preprocessAttachments, processDocs, isLocalId, parseDoc } from 'pouchdb-adapter-utils';\nimport { safeJsonParse, safeJsonStringify } from 'pouchdb-json';\nimport { compactTree, collectConflicts, isDeleted, isLocalId as isLocalId$1, traverseRevTree, winningRev, latest } from 'pouchdb-merge';\nimport { btoa, readAsBinaryString, base64StringToBlobOrBuffer, blob } from 'pouchdb-binary-utils';\nimport { createError, IDB_ERROR, MISSING_STUB, MISSING_DOC, REV_CONFLICT } from 'pouchdb-errors';\nimport { Map, Set } from 'pouchdb-collections';\nimport { assign, pick, changesHandler, nextTick, clone, filterChange, uuid, guardedConsole, toPromise, hasLocalStorage } from 'pouchdb-utils';\n\n// IndexedDB requires a versioned database structure, so we use the\n// version here to manage migrations.\nvar ADAPTER_VERSION = 5;\n\n// The object stores created for each database\n// DOC_STORE stores the document meta data, its revision history and state\n// Keyed by document id\nvar DOC_STORE = 'document-store';\n// BY_SEQ_STORE stores a particular version of a document, keyed by its\n// sequence id\nvar BY_SEQ_STORE = 'by-sequence';\n// Where we store attachments\nvar ATTACH_STORE = 'attach-store';\n// Where we store many-to-many relations\n// between attachment digests and seqs\nvar ATTACH_AND_SEQ_STORE = 'attach-seq-store';\n\n// Where we store database-wide meta data in a single record\n// keyed by id: META_STORE\nvar META_STORE = 'meta-store';\n// Where we store local documents\nvar LOCAL_STORE = 'local-store';\n// Where we detect blob support\nvar DETECT_BLOB_SUPPORT_STORE = 'detect-blob-support';\n\nfunction idbError(callback) {\n  return function (evt) {\n    var message = 'unknown_error';\n    if (evt.target && evt.target.error) {\n      message = evt.target.error.name || evt.target.error.message;\n    }\n    callback(createError(IDB_ERROR, message, evt.type));\n  };\n}\n\n// Unfortunately, the metadata has to be stringified\n// when it is put into the database, because otherwise\n// IndexedDB can throw errors for deeply-nested objects.\n// Originally we just used JSON.parse/JSON.stringify; now\n// we use this custom vuvuzela library that avoids recursion.\n// If we could do it all over again, we'd probably use a\n// format for the revision trees other than JSON.\nfunction encodeMetadata(metadata, winningRev$$1, deleted) {\n  return {\n    data: safeJsonStringify(metadata),\n    winningRev: winningRev$$1,\n    deletedOrLocal: deleted ? '1' : '0',\n    seq: metadata.seq, // highest seq for this doc\n    id: metadata.id\n  };\n}\n\nfunction decodeMetadata(storedObject) {\n  if (!storedObject) {\n    return null;\n  }\n  var metadata = safeJsonParse(storedObject.data);\n  metadata.winningRev = storedObject.winningRev;\n  metadata.deleted = storedObject.deletedOrLocal === '1';\n  metadata.seq = storedObject.seq;\n  return metadata;\n}\n\n// read the doc back out from the database. we don't store the\n// _id or _rev because we already have _doc_id_rev.\nfunction decodeDoc(doc) {\n  if (!doc) {\n    return doc;\n  }\n  var idx = doc._doc_id_rev.lastIndexOf(':');\n  doc._id = doc._doc_id_rev.substring(0, idx - 1);\n  doc._rev = doc._doc_id_rev.substring(idx + 1);\n  delete doc._doc_id_rev;\n  return doc;\n}\n\n// Read a blob from the database, encoding as necessary\n// and translating from base64 if the IDB doesn't support\n// native Blobs\nfunction readBlobData(body, type, asBlob, callback) {\n  if (asBlob) {\n    if (!body) {\n      callback(blob([''], {type: type}));\n    } else if (typeof body !== 'string') { // we have blob support\n      callback(body);\n    } else { // no blob support\n      callback(base64StringToBlobOrBuffer(body, type));\n    }\n  } else { // as base64 string\n    if (!body) {\n      callback('');\n    } else if (typeof body !== 'string') { // we have blob support\n      readAsBinaryString(body, function (binary) {\n        callback(btoa(binary));\n      });\n    } else { // no blob support\n      callback(body);\n    }\n  }\n}\n\nfunction fetchAttachmentsIfNecessary(doc, opts, txn, cb) {\n  var attachments = Object.keys(doc._attachments || {});\n  if (!attachments.length) {\n    return cb && cb();\n  }\n  var numDone = 0;\n\n  function checkDone() {\n    if (++numDone === attachments.length && cb) {\n      cb();\n    }\n  }\n\n  function fetchAttachment(doc, att) {\n    var attObj = doc._attachments[att];\n    var digest = attObj.digest;\n    var req = txn.objectStore(ATTACH_STORE).get(digest);\n    req.onsuccess = function (e) {\n      attObj.body = e.target.result.body;\n      checkDone();\n    };\n  }\n\n  attachments.forEach(function (att) {\n    if (opts.attachments && opts.include_docs) {\n      fetchAttachment(doc, att);\n    } else {\n      doc._attachments[att].stub = true;\n      checkDone();\n    }\n  });\n}\n\n// IDB-specific postprocessing necessary because\n// we don't know whether we stored a true Blob or\n// a base64-encoded string, and if it's a Blob it\n// needs to be read outside of the transaction context\nfunction postProcessAttachments(results, asBlob) {\n  return Promise.all(results.map(function (row) {\n    if (row.doc && row.doc._attachments) {\n      var attNames = Object.keys(row.doc._attachments);\n      return Promise.all(attNames.map(function (att) {\n        var attObj = row.doc._attachments[att];\n        if (!('body' in attObj)) { // already processed\n          return;\n        }\n        var body = attObj.body;\n        var type = attObj.content_type;\n        return new Promise(function (resolve) {\n          readBlobData(body, type, asBlob, function (data) {\n            row.doc._attachments[att] = assign(\n              pick(attObj, ['digest', 'content_type']),\n              {data: data}\n            );\n            resolve();\n          });\n        });\n      }));\n    }\n  }));\n}\n\nfunction compactRevs(revs, docId, txn) {\n\n  var possiblyOrphanedDigests = [];\n  var seqStore = txn.objectStore(BY_SEQ_STORE);\n  var attStore = txn.objectStore(ATTACH_STORE);\n  var attAndSeqStore = txn.objectStore(ATTACH_AND_SEQ_STORE);\n  var count = revs.length;\n\n  function checkDone() {\n    count--;\n    if (!count) { // done processing all revs\n      deleteOrphanedAttachments();\n    }\n  }\n\n  function deleteOrphanedAttachments() {\n    if (!possiblyOrphanedDigests.length) {\n      return;\n    }\n    possiblyOrphanedDigests.forEach(function (digest) {\n      var countReq = attAndSeqStore.index('digestSeq').count(\n        IDBKeyRange.bound(\n          digest + '::', digest + '::\\uffff', false, false));\n      countReq.onsuccess = function (e) {\n        var count = e.target.result;\n        if (!count) {\n          // orphaned\n          attStore.delete(digest);\n        }\n      };\n    });\n  }\n\n  revs.forEach(function (rev) {\n    var index = seqStore.index('_doc_id_rev');\n    var key = docId + \"::\" + rev;\n    index.getKey(key).onsuccess = function (e) {\n      var seq = e.target.result;\n      if (typeof seq !== 'number') {\n        return checkDone();\n      }\n      seqStore.delete(seq);\n\n      var cursor = attAndSeqStore.index('seq')\n        .openCursor(IDBKeyRange.only(seq));\n\n      cursor.onsuccess = function (event) {\n        var cursor = event.target.result;\n        if (cursor) {\n          var digest = cursor.value.digestSeq.split('::')[0];\n          possiblyOrphanedDigests.push(digest);\n          attAndSeqStore.delete(cursor.primaryKey);\n          cursor.continue();\n        } else { // done\n          checkDone();\n        }\n      };\n    };\n  });\n}\n\nfunction openTransactionSafely(idb, stores, mode) {\n  try {\n    return {\n      txn: idb.transaction(stores, mode)\n    };\n  } catch (err) {\n    return {\n      error: err\n    };\n  }\n}\n\nvar changesHandler$1 = new changesHandler();\n\nfunction idbBulkDocs(dbOpts, req, opts, api, idb, callback) {\n  var docInfos = req.docs;\n  var txn;\n  var docStore;\n  var bySeqStore;\n  var attachStore;\n  var attachAndSeqStore;\n  var metaStore;\n  var docInfoError;\n  var metaDoc;\n\n  for (var i = 0, len = docInfos.length; i < len; i++) {\n    var doc = docInfos[i];\n    if (doc._id && isLocalId(doc._id)) {\n      continue;\n    }\n    doc = docInfos[i] = parseDoc(doc, opts.new_edits, dbOpts);\n    if (doc.error && !docInfoError) {\n      docInfoError = doc;\n    }\n  }\n\n  if (docInfoError) {\n    return callback(docInfoError);\n  }\n\n  var allDocsProcessed = false;\n  var docCountDelta = 0;\n  var results = new Array(docInfos.length);\n  var fetchedDocs = new Map();\n  var preconditionErrored = false;\n  var blobType = api._meta.blobSupport ? 'blob' : 'base64';\n\n  preprocessAttachments(docInfos, blobType, function (err) {\n    if (err) {\n      return callback(err);\n    }\n    startTransaction();\n  });\n\n  function startTransaction() {\n\n    var stores = [\n      DOC_STORE, BY_SEQ_STORE,\n      ATTACH_STORE,\n      LOCAL_STORE, ATTACH_AND_SEQ_STORE,\n      META_STORE\n    ];\n    var txnResult = openTransactionSafely(idb, stores, 'readwrite');\n    if (txnResult.error) {\n      return callback(txnResult.error);\n    }\n    txn = txnResult.txn;\n    txn.onabort = idbError(callback);\n    txn.ontimeout = idbError(callback);\n    txn.oncomplete = complete;\n    docStore = txn.objectStore(DOC_STORE);\n    bySeqStore = txn.objectStore(BY_SEQ_STORE);\n    attachStore = txn.objectStore(ATTACH_STORE);\n    attachAndSeqStore = txn.objectStore(ATTACH_AND_SEQ_STORE);\n    metaStore = txn.objectStore(META_STORE);\n\n    metaStore.get(META_STORE).onsuccess = function (e) {\n      metaDoc = e.target.result;\n      updateDocCountIfReady();\n    };\n\n    verifyAttachments(function (err) {\n      if (err) {\n        preconditionErrored = true;\n        return callback(err);\n      }\n      fetchExistingDocs();\n    });\n  }\n\n  function onAllDocsProcessed() {\n    allDocsProcessed = true;\n    updateDocCountIfReady();\n  }\n\n  function idbProcessDocs() {\n    processDocs(dbOpts.revs_limit, docInfos, api, fetchedDocs,\n                txn, results, writeDoc, opts, onAllDocsProcessed);\n  }\n\n  function updateDocCountIfReady() {\n    if (!metaDoc || !allDocsProcessed) {\n      return;\n    }\n    // caching the docCount saves a lot of time in allDocs() and\n    // info(), which is why we go to all the trouble of doing this\n    metaDoc.docCount += docCountDelta;\n    metaStore.put(metaDoc);\n  }\n\n  function fetchExistingDocs() {\n\n    if (!docInfos.length) {\n      return;\n    }\n\n    var numFetched = 0;\n\n    function checkDone() {\n      if (++numFetched === docInfos.length) {\n        idbProcessDocs();\n      }\n    }\n\n    function readMetadata(event) {\n      var metadata = decodeMetadata(event.target.result);\n\n      if (metadata) {\n        fetchedDocs.set(metadata.id, metadata);\n      }\n      checkDone();\n    }\n\n    for (var i = 0, len = docInfos.length; i < len; i++) {\n      var docInfo = docInfos[i];\n      if (docInfo._id && isLocalId(docInfo._id)) {\n        checkDone(); // skip local docs\n        continue;\n      }\n      var req = docStore.get(docInfo.metadata.id);\n      req.onsuccess = readMetadata;\n    }\n  }\n\n  function complete() {\n    if (preconditionErrored) {\n      return;\n    }\n\n    changesHandler$1.notify(api._meta.name);\n    callback(null, results);\n  }\n\n  function verifyAttachment(digest, callback) {\n\n    var req = attachStore.get(digest);\n    req.onsuccess = function (e) {\n      if (!e.target.result) {\n        var err = createError(MISSING_STUB,\n          'unknown stub attachment with digest ' +\n          digest);\n        err.status = 412;\n        callback(err);\n      } else {\n        callback();\n      }\n    };\n  }\n\n  function verifyAttachments(finish) {\n\n\n    var digests = [];\n    docInfos.forEach(function (docInfo) {\n      if (docInfo.data && docInfo.data._attachments) {\n        Object.keys(docInfo.data._attachments).forEach(function (filename) {\n          var att = docInfo.data._attachments[filename];\n          if (att.stub) {\n            digests.push(att.digest);\n          }\n        });\n      }\n    });\n    if (!digests.length) {\n      return finish();\n    }\n    var numDone = 0;\n    var err;\n\n    function checkDone() {\n      if (++numDone === digests.length) {\n        finish(err);\n      }\n    }\n    digests.forEach(function (digest) {\n      verifyAttachment(digest, function (attErr) {\n        if (attErr && !err) {\n          err = attErr;\n        }\n        checkDone();\n      });\n    });\n  }\n\n  function writeDoc(docInfo, winningRev$$1, winningRevIsDeleted, newRevIsDeleted,\n                    isUpdate, delta, resultsIdx, callback) {\n\n    docInfo.metadata.winningRev = winningRev$$1;\n    docInfo.metadata.deleted = winningRevIsDeleted;\n\n    var doc = docInfo.data;\n    doc._id = docInfo.metadata.id;\n    doc._rev = docInfo.metadata.rev;\n\n    if (newRevIsDeleted) {\n      doc._deleted = true;\n    }\n\n    var hasAttachments = doc._attachments &&\n      Object.keys(doc._attachments).length;\n    if (hasAttachments) {\n      return writeAttachments(docInfo, winningRev$$1, winningRevIsDeleted,\n        isUpdate, resultsIdx, callback);\n    }\n\n    docCountDelta += delta;\n    updateDocCountIfReady();\n\n    finishDoc(docInfo, winningRev$$1, winningRevIsDeleted,\n      isUpdate, resultsIdx, callback);\n  }\n\n  function finishDoc(docInfo, winningRev$$1, winningRevIsDeleted,\n                     isUpdate, resultsIdx, callback) {\n\n    var doc = docInfo.data;\n    var metadata = docInfo.metadata;\n\n    doc._doc_id_rev = metadata.id + '::' + metadata.rev;\n    delete doc._id;\n    delete doc._rev;\n\n    function afterPutDoc(e) {\n      var revsToDelete = docInfo.stemmedRevs || [];\n\n      if (isUpdate && api.auto_compaction) {\n        revsToDelete = revsToDelete.concat(compactTree(docInfo.metadata));\n      }\n\n      if (revsToDelete && revsToDelete.length) {\n        compactRevs(revsToDelete, docInfo.metadata.id, txn);\n      }\n\n      metadata.seq = e.target.result;\n      // Current _rev is calculated from _rev_tree on read\n      // delete metadata.rev;\n      var metadataToStore = encodeMetadata(metadata, winningRev$$1,\n        winningRevIsDeleted);\n      var metaDataReq = docStore.put(metadataToStore);\n      metaDataReq.onsuccess = afterPutMetadata;\n    }\n\n    function afterPutDocError(e) {\n      // ConstraintError, need to update, not put (see #1638 for details)\n      e.preventDefault(); // avoid transaction abort\n      e.stopPropagation(); // avoid transaction onerror\n      var index = bySeqStore.index('_doc_id_rev');\n      var getKeyReq = index.getKey(doc._doc_id_rev);\n      getKeyReq.onsuccess = function (e) {\n        var putReq = bySeqStore.put(doc, e.target.result);\n        putReq.onsuccess = afterPutDoc;\n      };\n    }\n\n    function afterPutMetadata() {\n      results[resultsIdx] = {\n        ok: true,\n        id: metadata.id,\n        rev: metadata.rev\n      };\n      fetchedDocs.set(docInfo.metadata.id, docInfo.metadata);\n      insertAttachmentMappings(docInfo, metadata.seq, callback);\n    }\n\n    var putReq = bySeqStore.put(doc);\n\n    putReq.onsuccess = afterPutDoc;\n    putReq.onerror = afterPutDocError;\n  }\n\n  function writeAttachments(docInfo, winningRev$$1, winningRevIsDeleted,\n                            isUpdate, resultsIdx, callback) {\n\n\n    var doc = docInfo.data;\n\n    var numDone = 0;\n    var attachments = Object.keys(doc._attachments);\n\n    function collectResults() {\n      if (numDone === attachments.length) {\n        finishDoc(docInfo, winningRev$$1, winningRevIsDeleted,\n          isUpdate, resultsIdx, callback);\n      }\n    }\n\n    function attachmentSaved() {\n      numDone++;\n      collectResults();\n    }\n\n    attachments.forEach(function (key) {\n      var att = docInfo.data._attachments[key];\n      if (!att.stub) {\n        var data = att.data;\n        delete att.data;\n        att.revpos = parseInt(winningRev$$1, 10);\n        var digest = att.digest;\n        saveAttachment(digest, data, attachmentSaved);\n      } else {\n        numDone++;\n        collectResults();\n      }\n    });\n  }\n\n  // map seqs to attachment digests, which\n  // we will need later during compaction\n  function insertAttachmentMappings(docInfo, seq, callback) {\n\n    var attsAdded = 0;\n    var attsToAdd = Object.keys(docInfo.data._attachments || {});\n\n    if (!attsToAdd.length) {\n      return callback();\n    }\n\n    function checkDone() {\n      if (++attsAdded === attsToAdd.length) {\n        callback();\n      }\n    }\n\n    function add(att) {\n      var digest = docInfo.data._attachments[att].digest;\n      var req = attachAndSeqStore.put({\n        seq: seq,\n        digestSeq: digest + '::' + seq\n      });\n\n      req.onsuccess = checkDone;\n      req.onerror = function (e) {\n        // this callback is for a constaint error, which we ignore\n        // because this docid/rev has already been associated with\n        // the digest (e.g. when new_edits == false)\n        e.preventDefault(); // avoid transaction abort\n        e.stopPropagation(); // avoid transaction onerror\n        checkDone();\n      };\n    }\n    for (var i = 0; i < attsToAdd.length; i++) {\n      add(attsToAdd[i]); // do in parallel\n    }\n  }\n\n  function saveAttachment(digest, data, callback) {\n\n\n    var getKeyReq = attachStore.count(digest);\n    getKeyReq.onsuccess = function (e) {\n      var count = e.target.result;\n      if (count) {\n        return callback(); // already exists\n      }\n      var newAtt = {\n        digest: digest,\n        body: data\n      };\n      var putReq = attachStore.put(newAtt);\n      putReq.onsuccess = callback;\n    };\n  }\n}\n\n// Abstraction over IDBCursor and getAll()/getAllKeys() that allows us to batch our operations\n// while falling back to a normal IDBCursor operation on browsers that don't support getAll() or\n// getAllKeys(). This allows for a much faster implementation than just straight-up cursors, because\n// we're not processing each document one-at-a-time.\nfunction runBatchedCursor(objectStore, keyRange, descending, batchSize, onBatch) {\n\n  if (batchSize === -1) {\n    batchSize = 1000;\n  }\n\n  // Bail out of getAll()/getAllKeys() in the following cases:\n  // 1) either method is unsupported - we need both\n  // 2) batchSize is 1 (might as well use IDBCursor)\n  // 3) descending â€“ no real way to do this via getAll()/getAllKeys()\n\n  var useGetAll = typeof objectStore.getAll === 'function' &&\n    typeof objectStore.getAllKeys === 'function' &&\n    batchSize > 1 && !descending;\n\n  var keysBatch;\n  var valuesBatch;\n  var pseudoCursor;\n\n  function onGetAll(e) {\n    valuesBatch = e.target.result;\n    if (keysBatch) {\n      onBatch(keysBatch, valuesBatch, pseudoCursor);\n    }\n  }\n\n  function onGetAllKeys(e) {\n    keysBatch = e.target.result;\n    if (valuesBatch) {\n      onBatch(keysBatch, valuesBatch, pseudoCursor);\n    }\n  }\n\n  function continuePseudoCursor() {\n    if (!keysBatch.length) { // no more results\n      return onBatch();\n    }\n    // fetch next batch, exclusive start\n    var lastKey = keysBatch[keysBatch.length - 1];\n    var newKeyRange;\n    if (keyRange && keyRange.upper) {\n      try {\n        newKeyRange = IDBKeyRange.bound(lastKey, keyRange.upper,\n          true, keyRange.upperOpen);\n      } catch (e) {\n        if (e.name === \"DataError\" && e.code === 0) {\n          return onBatch(); // we're done, startkey and endkey are equal\n        }\n      }\n    } else {\n      newKeyRange = IDBKeyRange.lowerBound(lastKey, true);\n    }\n    keyRange = newKeyRange;\n    keysBatch = null;\n    valuesBatch = null;\n    objectStore.getAll(keyRange, batchSize).onsuccess = onGetAll;\n    objectStore.getAllKeys(keyRange, batchSize).onsuccess = onGetAllKeys;\n  }\n\n  function onCursor(e) {\n    var cursor = e.target.result;\n    if (!cursor) { // done\n      return onBatch();\n    }\n    // regular IDBCursor acts like a batch where batch size is always 1\n    onBatch([cursor.key], [cursor.value], cursor);\n  }\n\n  if (useGetAll) {\n    pseudoCursor = {\"continue\": continuePseudoCursor};\n    objectStore.getAll(keyRange, batchSize).onsuccess = onGetAll;\n    objectStore.getAllKeys(keyRange, batchSize).onsuccess = onGetAllKeys;\n  } else if (descending) {\n    objectStore.openCursor(keyRange, 'prev').onsuccess = onCursor;\n  } else {\n    objectStore.openCursor(keyRange).onsuccess = onCursor;\n  }\n}\n\n// simple shim for objectStore.getAll(), falling back to IDBCursor\nfunction getAll(objectStore, keyRange, onSuccess) {\n  if (typeof objectStore.getAll === 'function') {\n    // use native getAll\n    objectStore.getAll(keyRange).onsuccess = onSuccess;\n    return;\n  }\n  // fall back to cursors\n  var values = [];\n\n  function onCursor(e) {\n    var cursor = e.target.result;\n    if (cursor) {\n      values.push(cursor.value);\n      cursor.continue();\n    } else {\n      onSuccess({\n        target: {\n          result: values\n        }\n      });\n    }\n  }\n\n  objectStore.openCursor(keyRange).onsuccess = onCursor;\n}\n\nfunction allDocsKeys(keys, docStore, onBatch) {\n  // It's not guaranted to be returned in right order  \n  var valuesBatch = new Array(keys.length);\n  var count = 0;\n  keys.forEach(function (key, index) {\n    docStore.get(key).onsuccess = function (event) {\n      if (event.target.result) {\n        valuesBatch[index] = event.target.result;\n      } else {\n        valuesBatch[index] = {key: key, error: 'not_found'};\n      }\n      count++;\n      if (count === keys.length) {\n        onBatch(keys, valuesBatch, {});\n      }\n    };\n  });\n}\n\nfunction createKeyRange(start, end, inclusiveEnd, key, descending) {\n  try {\n    if (start && end) {\n      if (descending) {\n        return IDBKeyRange.bound(end, start, !inclusiveEnd, false);\n      } else {\n        return IDBKeyRange.bound(start, end, false, !inclusiveEnd);\n      }\n    } else if (start) {\n      if (descending) {\n        return IDBKeyRange.upperBound(start);\n      } else {\n        return IDBKeyRange.lowerBound(start);\n      }\n    } else if (end) {\n      if (descending) {\n        return IDBKeyRange.lowerBound(end, !inclusiveEnd);\n      } else {\n        return IDBKeyRange.upperBound(end, !inclusiveEnd);\n      }\n    } else if (key) {\n      return IDBKeyRange.only(key);\n    }\n  } catch (e) {\n    return {error: e};\n  }\n  return null;\n}\n\nfunction idbAllDocs(opts, idb, callback) {\n  var start = 'startkey' in opts ? opts.startkey : false;\n  var end = 'endkey' in opts ? opts.endkey : false;\n  var key = 'key' in opts ? opts.key : false;\n  var keys = 'keys' in opts ? opts.keys : false; \n  var skip = opts.skip || 0;\n  var limit = typeof opts.limit === 'number' ? opts.limit : -1;\n  var inclusiveEnd = opts.inclusive_end !== false;\n\n  var keyRange ; \n  var keyRangeError;\n  if (!keys) {\n    keyRange = createKeyRange(start, end, inclusiveEnd, key, opts.descending);\n    keyRangeError = keyRange && keyRange.error;\n    if (keyRangeError && \n      !(keyRangeError.name === \"DataError\" && keyRangeError.code === 0)) {\n      // DataError with error code 0 indicates start is less than end, so\n      // can just do an empty query. Else need to throw\n      return callback(createError(IDB_ERROR,\n        keyRangeError.name, keyRangeError.message));\n    }\n  }\n\n  var stores = [DOC_STORE, BY_SEQ_STORE, META_STORE];\n\n  if (opts.attachments) {\n    stores.push(ATTACH_STORE);\n  }\n  var txnResult = openTransactionSafely(idb, stores, 'readonly');\n  if (txnResult.error) {\n    return callback(txnResult.error);\n  }\n  var txn = txnResult.txn;\n  txn.oncomplete = onTxnComplete;\n  txn.onabort = idbError(callback);\n  var docStore = txn.objectStore(DOC_STORE);\n  var seqStore = txn.objectStore(BY_SEQ_STORE);\n  var metaStore = txn.objectStore(META_STORE);\n  var docIdRevIndex = seqStore.index('_doc_id_rev');\n  var results = [];\n  var docCount;\n  var updateSeq;\n\n  metaStore.get(META_STORE).onsuccess = function (e) {\n    docCount = e.target.result.docCount;\n  };\n\n  /* istanbul ignore if */\n  if (opts.update_seq) {\n    getMaxUpdateSeq(seqStore, function (e) { \n      if (e.target.result && e.target.result.length > 0) {\n        updateSeq = e.target.result[0];\n      }\n    });\n  }\n\n  function getMaxUpdateSeq(objectStore, onSuccess) {\n    function onCursor(e) {\n      var cursor = e.target.result;\n      var maxKey = undefined;\n      if (cursor && cursor.key) {\n        maxKey = cursor.key;\n      } \n      return onSuccess({\n        target: {\n          result: [maxKey]\n        }\n      });\n    }\n    objectStore.openCursor(null, 'prev').onsuccess = onCursor;\n  }\n\n  // if the user specifies include_docs=true, then we don't\n  // want to block the main cursor while we're fetching the doc\n  function fetchDocAsynchronously(metadata, row, winningRev$$1) {\n    var key = metadata.id + \"::\" + winningRev$$1;\n    docIdRevIndex.get(key).onsuccess =  function onGetDoc(e) {\n      row.doc = decodeDoc(e.target.result) || {};\n      if (opts.conflicts) {\n        var conflicts = collectConflicts(metadata);\n        if (conflicts.length) {\n          row.doc._conflicts = conflicts;\n        }\n      }\n      fetchAttachmentsIfNecessary(row.doc, opts, txn);\n    };\n  }\n\n  function allDocsInner(winningRev$$1, metadata) {\n    var row = {\n      id: metadata.id,\n      key: metadata.id,\n      value: {\n        rev: winningRev$$1\n      }\n    };\n    var deleted = metadata.deleted;\n    if (deleted) {\n      if (keys) {\n        results.push(row);\n        // deleted docs are okay with \"keys\" requests\n        row.value.deleted = true;\n        row.doc = null;\n      }\n    } else if (skip-- <= 0) {\n      results.push(row);\n      if (opts.include_docs) {\n        fetchDocAsynchronously(metadata, row, winningRev$$1);\n      }\n    }\n  }\n\n  function processBatch(batchValues) {\n    for (var i = 0, len = batchValues.length; i < len; i++) {\n      if (results.length === limit) {\n        break;\n      }\n      var batchValue = batchValues[i];\n      if (batchValue.error && keys) {\n        // key was not found with \"keys\" requests\n        results.push(batchValue);\n        continue;\n      }\n      var metadata = decodeMetadata(batchValue);\n      var winningRev$$1 = metadata.winningRev;\n      allDocsInner(winningRev$$1, metadata);\n    }\n  }\n\n  function onBatch(batchKeys, batchValues, cursor) {\n    if (!cursor) {\n      return;\n    }\n    processBatch(batchValues);\n    if (results.length < limit) {\n      cursor.continue();\n    }\n  }\n\n  function onGetAll(e) {\n    var values = e.target.result;\n    if (opts.descending) {\n      values = values.reverse();\n    }\n    processBatch(values);\n  }\n\n  function onResultsReady() {\n    var returnVal = {\n      total_rows: docCount,\n      offset: opts.skip,\n      rows: results\n    };\n    \n    /* istanbul ignore if */\n    if (opts.update_seq && updateSeq !== undefined) {\n      returnVal.update_seq = updateSeq;\n    }\n    callback(null, returnVal);\n  }\n\n  function onTxnComplete() {\n    if (opts.attachments) {\n      postProcessAttachments(results, opts.binary).then(onResultsReady);\n    } else {\n      onResultsReady();\n    }\n  }\n\n  // don't bother doing any requests if start > end or limit === 0\n  if (keyRangeError || limit === 0) {\n    return;\n  }\n  if (keys) {\n    return allDocsKeys(opts.keys, docStore, onBatch);\n  }\n  if (limit === -1) { // just fetch everything\n    return getAll(docStore, keyRange, onGetAll);\n  }\n  // else do a cursor\n  // choose a batch size based on the skip, since we'll need to skip that many\n  runBatchedCursor(docStore, keyRange, opts.descending, limit + skip, onBatch);\n}\n\n//\n// Blobs are not supported in all versions of IndexedDB, notably\n// Chrome <37 and Android <5. In those versions, storing a blob will throw.\n//\n// Various other blob bugs exist in Chrome v37-42 (inclusive).\n// Detecting them is expensive and confusing to users, and Chrome 37-42\n// is at very low usage worldwide, so we do a hacky userAgent check instead.\n//\n// content-type bug: https://code.google.com/p/chromium/issues/detail?id=408120\n// 404 bug: https://code.google.com/p/chromium/issues/detail?id=447916\n// FileReader bug: https://code.google.com/p/chromium/issues/detail?id=447836\n//\nfunction checkBlobSupport(txn) {\n  return new Promise(function (resolve) {\n    var blob$$1 = blob(['']);\n    var req = txn.objectStore(DETECT_BLOB_SUPPORT_STORE).put(blob$$1, 'key');\n\n    req.onsuccess = function () {\n      var matchedChrome = navigator.userAgent.match(/Chrome\\/(\\d+)/);\n      var matchedEdge = navigator.userAgent.match(/Edge\\//);\n      // MS Edge pretends to be Chrome 42:\n      // https://msdn.microsoft.com/en-us/library/hh869301%28v=vs.85%29.aspx\n      resolve(matchedEdge || !matchedChrome ||\n        parseInt(matchedChrome[1], 10) >= 43);\n    };\n\n    req.onerror = txn.onabort = function (e) {\n      // If the transaction aborts now its due to not being able to\n      // write to the database, likely due to the disk being full\n      e.preventDefault();\n      e.stopPropagation();\n      resolve(false);\n    };\n  }).catch(function () {\n    return false; // error, so assume unsupported\n  });\n}\n\nfunction countDocs(txn, cb) {\n  var index = txn.objectStore(DOC_STORE).index('deletedOrLocal');\n  index.count(IDBKeyRange.only('0')).onsuccess = function (e) {\n    cb(e.target.result);\n  };\n}\n\n// This task queue ensures that IDB open calls are done in their own tick\n\nvar running = false;\nvar queue = [];\n\nfunction tryCode(fun, err, res, PouchDB) {\n  try {\n    fun(err, res);\n  } catch (err) {\n    // Shouldn't happen, but in some odd cases\n    // IndexedDB implementations might throw a sync\n    // error, in which case this will at least log it.\n    PouchDB.emit('error', err);\n  }\n}\n\nfunction applyNext() {\n  if (running || !queue.length) {\n    return;\n  }\n  running = true;\n  queue.shift()();\n}\n\nfunction enqueueTask(action, callback, PouchDB) {\n  queue.push(function runAction() {\n    action(function runCallback(err, res) {\n      tryCode(callback, err, res, PouchDB);\n      running = false;\n      nextTick(function runNext() {\n        applyNext(PouchDB);\n      });\n    });\n  });\n  applyNext();\n}\n\nfunction changes(opts, api, dbName, idb) {\n  opts = clone(opts);\n\n  if (opts.continuous) {\n    var id = dbName + ':' + uuid();\n    changesHandler$1.addListener(dbName, id, api, opts);\n    changesHandler$1.notify(dbName);\n    return {\n      cancel: function () {\n        changesHandler$1.removeListener(dbName, id);\n      }\n    };\n  }\n\n  var docIds = opts.doc_ids && new Set(opts.doc_ids);\n\n  opts.since = opts.since || 0;\n  var lastSeq = opts.since;\n\n  var limit = 'limit' in opts ? opts.limit : -1;\n  if (limit === 0) {\n    limit = 1; // per CouchDB _changes spec\n  }\n\n  var results = [];\n  var numResults = 0;\n  var filter = filterChange(opts);\n  var docIdsToMetadata = new Map();\n\n  var txn;\n  var bySeqStore;\n  var docStore;\n  var docIdRevIndex;\n\n  function onBatch(batchKeys, batchValues, cursor) {\n    if (!cursor || !batchKeys.length) { // done\n      return;\n    }\n\n    var winningDocs = new Array(batchKeys.length);\n    var metadatas = new Array(batchKeys.length);\n\n    function processMetadataAndWinningDoc(metadata, winningDoc) {\n      var change = opts.processChange(winningDoc, metadata, opts);\n      lastSeq = change.seq = metadata.seq;\n\n      var filtered = filter(change);\n      if (typeof filtered === 'object') { // anything but true/false indicates error\n        return Promise.reject(filtered);\n      }\n\n      if (!filtered) {\n        return Promise.resolve();\n      }\n      numResults++;\n      if (opts.return_docs) {\n        results.push(change);\n      }\n      // process the attachment immediately\n      // for the benefit of live listeners\n      if (opts.attachments && opts.include_docs) {\n        return new Promise(function (resolve) {\n          fetchAttachmentsIfNecessary(winningDoc, opts, txn, function () {\n            postProcessAttachments([change], opts.binary).then(function () {\n              resolve(change);\n            });\n          });\n        });\n      } else {\n        return Promise.resolve(change);\n      }\n    }\n\n    function onBatchDone() {\n      var promises = [];\n      for (var i = 0, len = winningDocs.length; i < len; i++) {\n        if (numResults === limit) {\n          break;\n        }\n        var winningDoc = winningDocs[i];\n        if (!winningDoc) {\n          continue;\n        }\n        var metadata = metadatas[i];\n        promises.push(processMetadataAndWinningDoc(metadata, winningDoc));\n      }\n\n      Promise.all(promises).then(function (changes) {\n        for (var i = 0, len = changes.length; i < len; i++) {\n          if (changes[i]) {\n            opts.onChange(changes[i]);\n          }\n        }\n      }).catch(opts.complete);\n\n      if (numResults !== limit) {\n        cursor.continue();\n      }\n    }\n\n    // Fetch all metadatas/winningdocs from this batch in parallel, then process\n    // them all only once all data has been collected. This is done in parallel\n    // because it's faster than doing it one-at-a-time.\n    var numDone = 0;\n    batchValues.forEach(function (value, i) {\n      var doc = decodeDoc(value);\n      var seq = batchKeys[i];\n      fetchWinningDocAndMetadata(doc, seq, function (metadata, winningDoc) {\n        metadatas[i] = metadata;\n        winningDocs[i] = winningDoc;\n        if (++numDone === batchKeys.length) {\n          onBatchDone();\n        }\n      });\n    });\n  }\n\n  function onGetMetadata(doc, seq, metadata, cb) {\n    if (metadata.seq !== seq) {\n      // some other seq is later\n      return cb();\n    }\n\n    if (metadata.winningRev === doc._rev) {\n      // this is the winning doc\n      return cb(metadata, doc);\n    }\n\n    // fetch winning doc in separate request\n    var docIdRev = doc._id + '::' + metadata.winningRev;\n    var req = docIdRevIndex.get(docIdRev);\n    req.onsuccess = function (e) {\n      cb(metadata, decodeDoc(e.target.result));\n    };\n  }\n\n  function fetchWinningDocAndMetadata(doc, seq, cb) {\n    if (docIds && !docIds.has(doc._id)) {\n      return cb();\n    }\n\n    var metadata = docIdsToMetadata.get(doc._id);\n    if (metadata) { // cached\n      return onGetMetadata(doc, seq, metadata, cb);\n    }\n    // metadata not cached, have to go fetch it\n    docStore.get(doc._id).onsuccess = function (e) {\n      metadata = decodeMetadata(e.target.result);\n      docIdsToMetadata.set(doc._id, metadata);\n      onGetMetadata(doc, seq, metadata, cb);\n    };\n  }\n\n  function finish() {\n    opts.complete(null, {\n      results: results,\n      last_seq: lastSeq\n    });\n  }\n\n  function onTxnComplete() {\n    if (!opts.continuous && opts.attachments) {\n      // cannot guarantee that postProcessing was already done,\n      // so do it again\n      postProcessAttachments(results).then(finish);\n    } else {\n      finish();\n    }\n  }\n\n  var objectStores = [DOC_STORE, BY_SEQ_STORE];\n  if (opts.attachments) {\n    objectStores.push(ATTACH_STORE);\n  }\n  var txnResult = openTransactionSafely(idb, objectStores, 'readonly');\n  if (txnResult.error) {\n    return opts.complete(txnResult.error);\n  }\n  txn = txnResult.txn;\n  txn.onabort = idbError(opts.complete);\n  txn.oncomplete = onTxnComplete;\n\n  bySeqStore = txn.objectStore(BY_SEQ_STORE);\n  docStore = txn.objectStore(DOC_STORE);\n  docIdRevIndex = bySeqStore.index('_doc_id_rev');\n\n  var keyRange = (opts.since && !opts.descending) ?\n    IDBKeyRange.lowerBound(opts.since, true) : null;\n\n  runBatchedCursor(bySeqStore, keyRange, opts.descending, limit, onBatch);\n}\n\nvar cachedDBs = new Map();\nvar blobSupportPromise;\nvar openReqList = new Map();\n\nfunction IdbPouch(opts, callback) {\n  var api = this;\n\n  enqueueTask(function (thisCallback) {\n    init(api, opts, thisCallback);\n  }, callback, api.constructor);\n}\n\nfunction init(api, opts, callback) {\n\n  var dbName = opts.name;\n\n  var idb = null;\n  api._meta = null;\n\n  // called when creating a fresh new database\n  function createSchema(db) {\n    var docStore = db.createObjectStore(DOC_STORE, {keyPath : 'id'});\n    db.createObjectStore(BY_SEQ_STORE, {autoIncrement: true})\n      .createIndex('_doc_id_rev', '_doc_id_rev', {unique: true});\n    db.createObjectStore(ATTACH_STORE, {keyPath: 'digest'});\n    db.createObjectStore(META_STORE, {keyPath: 'id', autoIncrement: false});\n    db.createObjectStore(DETECT_BLOB_SUPPORT_STORE);\n\n    // added in v2\n    docStore.createIndex('deletedOrLocal', 'deletedOrLocal', {unique : false});\n\n    // added in v3\n    db.createObjectStore(LOCAL_STORE, {keyPath: '_id'});\n\n    // added in v4\n    var attAndSeqStore = db.createObjectStore(ATTACH_AND_SEQ_STORE,\n      {autoIncrement: true});\n    attAndSeqStore.createIndex('seq', 'seq');\n    attAndSeqStore.createIndex('digestSeq', 'digestSeq', {unique: true});\n  }\n\n  // migration to version 2\n  // unfortunately \"deletedOrLocal\" is a misnomer now that we no longer\n  // store local docs in the main doc-store, but whaddyagonnado\n  function addDeletedOrLocalIndex(txn, callback) {\n    var docStore = txn.objectStore(DOC_STORE);\n    docStore.createIndex('deletedOrLocal', 'deletedOrLocal', {unique : false});\n\n    docStore.openCursor().onsuccess = function (event) {\n      var cursor = event.target.result;\n      if (cursor) {\n        var metadata = cursor.value;\n        var deleted = isDeleted(metadata);\n        metadata.deletedOrLocal = deleted ? \"1\" : \"0\";\n        docStore.put(metadata);\n        cursor.continue();\n      } else {\n        callback();\n      }\n    };\n  }\n\n  // migration to version 3 (part 1)\n  function createLocalStoreSchema(db) {\n    db.createObjectStore(LOCAL_STORE, {keyPath: '_id'})\n      .createIndex('_doc_id_rev', '_doc_id_rev', {unique: true});\n  }\n\n  // migration to version 3 (part 2)\n  function migrateLocalStore(txn, cb) {\n    var localStore = txn.objectStore(LOCAL_STORE);\n    var docStore = txn.objectStore(DOC_STORE);\n    var seqStore = txn.objectStore(BY_SEQ_STORE);\n\n    var cursor = docStore.openCursor();\n    cursor.onsuccess = function (event) {\n      var cursor = event.target.result;\n      if (cursor) {\n        var metadata = cursor.value;\n        var docId = metadata.id;\n        var local = isLocalId$1(docId);\n        var rev = winningRev(metadata);\n        if (local) {\n          var docIdRev = docId + \"::\" + rev;\n          // remove all seq entries\n          // associated with this docId\n          var start = docId + \"::\";\n          var end = docId + \"::~\";\n          var index = seqStore.index('_doc_id_rev');\n          var range = IDBKeyRange.bound(start, end, false, false);\n          var seqCursor = index.openCursor(range);\n          seqCursor.onsuccess = function (e) {\n            seqCursor = e.target.result;\n            if (!seqCursor) {\n              // done\n              docStore.delete(cursor.primaryKey);\n              cursor.continue();\n            } else {\n              var data = seqCursor.value;\n              if (data._doc_id_rev === docIdRev) {\n                localStore.put(data);\n              }\n              seqStore.delete(seqCursor.primaryKey);\n              seqCursor.continue();\n            }\n          };\n        } else {\n          cursor.continue();\n        }\n      } else if (cb) {\n        cb();\n      }\n    };\n  }\n\n  // migration to version 4 (part 1)\n  function addAttachAndSeqStore(db) {\n    var attAndSeqStore = db.createObjectStore(ATTACH_AND_SEQ_STORE,\n      {autoIncrement: true});\n    attAndSeqStore.createIndex('seq', 'seq');\n    attAndSeqStore.createIndex('digestSeq', 'digestSeq', {unique: true});\n  }\n\n  // migration to version 4 (part 2)\n  function migrateAttsAndSeqs(txn, callback) {\n    var seqStore = txn.objectStore(BY_SEQ_STORE);\n    var attStore = txn.objectStore(ATTACH_STORE);\n    var attAndSeqStore = txn.objectStore(ATTACH_AND_SEQ_STORE);\n\n    // need to actually populate the table. this is the expensive part,\n    // so as an optimization, check first that this database even\n    // contains attachments\n    var req = attStore.count();\n    req.onsuccess = function (e) {\n      var count = e.target.result;\n      if (!count) {\n        return callback(); // done\n      }\n\n      seqStore.openCursor().onsuccess = function (e) {\n        var cursor = e.target.result;\n        if (!cursor) {\n          return callback(); // done\n        }\n        var doc = cursor.value;\n        var seq = cursor.primaryKey;\n        var atts = Object.keys(doc._attachments || {});\n        var digestMap = {};\n        for (var j = 0; j < atts.length; j++) {\n          var att = doc._attachments[atts[j]];\n          digestMap[att.digest] = true; // uniq digests, just in case\n        }\n        var digests = Object.keys(digestMap);\n        for (j = 0; j < digests.length; j++) {\n          var digest = digests[j];\n          attAndSeqStore.put({\n            seq: seq,\n            digestSeq: digest + '::' + seq\n          });\n        }\n        cursor.continue();\n      };\n    };\n  }\n\n  // migration to version 5\n  // Instead of relying on on-the-fly migration of metadata,\n  // this brings the doc-store to its modern form:\n  // - metadata.winningrev\n  // - metadata.seq\n  // - stringify the metadata when storing it\n  function migrateMetadata(txn) {\n\n    function decodeMetadataCompat(storedObject) {\n      if (!storedObject.data) {\n        // old format, when we didn't store it stringified\n        storedObject.deleted = storedObject.deletedOrLocal === '1';\n        return storedObject;\n      }\n      return decodeMetadata(storedObject);\n    }\n\n    // ensure that every metadata has a winningRev and seq,\n    // which was previously created on-the-fly but better to migrate\n    var bySeqStore = txn.objectStore(BY_SEQ_STORE);\n    var docStore = txn.objectStore(DOC_STORE);\n    var cursor = docStore.openCursor();\n    cursor.onsuccess = function (e) {\n      var cursor = e.target.result;\n      if (!cursor) {\n        return; // done\n      }\n      var metadata = decodeMetadataCompat(cursor.value);\n\n      metadata.winningRev = metadata.winningRev ||\n        winningRev(metadata);\n\n      function fetchMetadataSeq() {\n        // metadata.seq was added post-3.2.0, so if it's missing,\n        // we need to fetch it manually\n        var start = metadata.id + '::';\n        var end = metadata.id + '::\\uffff';\n        var req = bySeqStore.index('_doc_id_rev').openCursor(\n          IDBKeyRange.bound(start, end));\n\n        var metadataSeq = 0;\n        req.onsuccess = function (e) {\n          var cursor = e.target.result;\n          if (!cursor) {\n            metadata.seq = metadataSeq;\n            return onGetMetadataSeq();\n          }\n          var seq = cursor.primaryKey;\n          if (seq > metadataSeq) {\n            metadataSeq = seq;\n          }\n          cursor.continue();\n        };\n      }\n\n      function onGetMetadataSeq() {\n        var metadataToStore = encodeMetadata(metadata,\n          metadata.winningRev, metadata.deleted);\n\n        var req = docStore.put(metadataToStore);\n        req.onsuccess = function () {\n          cursor.continue();\n        };\n      }\n\n      if (metadata.seq) {\n        return onGetMetadataSeq();\n      }\n\n      fetchMetadataSeq();\n    };\n\n  }\n\n  api._remote = false;\n  api.type = function () {\n    return 'idb';\n  };\n\n  api._id = toPromise(function (callback) {\n    callback(null, api._meta.instanceId);\n  });\n\n  api._bulkDocs = function idb_bulkDocs(req, reqOpts, callback) {\n    idbBulkDocs(opts, req, reqOpts, api, idb, callback);\n  };\n\n  // First we look up the metadata in the ids database, then we fetch the\n  // current revision(s) from the by sequence store\n  api._get = function idb_get(id, opts, callback) {\n    var doc;\n    var metadata;\n    var err;\n    var txn = opts.ctx;\n    if (!txn) {\n      var txnResult = openTransactionSafely(idb,\n        [DOC_STORE, BY_SEQ_STORE, ATTACH_STORE], 'readonly');\n      if (txnResult.error) {\n        return callback(txnResult.error);\n      }\n      txn = txnResult.txn;\n    }\n\n    function finish() {\n      callback(err, {doc: doc, metadata: metadata, ctx: txn});\n    }\n\n    txn.objectStore(DOC_STORE).get(id).onsuccess = function (e) {\n      metadata = decodeMetadata(e.target.result);\n      // we can determine the result here if:\n      // 1. there is no such document\n      // 2. the document is deleted and we don't ask about specific rev\n      // When we ask with opts.rev we expect the answer to be either\n      // doc (possibly with _deleted=true) or missing error\n      if (!metadata) {\n        err = createError(MISSING_DOC, 'missing');\n        return finish();\n      }\n\n      var rev;\n      if (!opts.rev) {\n        rev = metadata.winningRev;\n        var deleted = isDeleted(metadata);\n        if (deleted) {\n          err = createError(MISSING_DOC, \"deleted\");\n          return finish();\n        }\n      } else {\n        rev = opts.latest ? latest(opts.rev, metadata) : opts.rev;\n      }\n\n      var objectStore = txn.objectStore(BY_SEQ_STORE);\n      var key = metadata.id + '::' + rev;\n\n      objectStore.index('_doc_id_rev').get(key).onsuccess = function (e) {\n        doc = e.target.result;\n        if (doc) {\n          doc = decodeDoc(doc);\n        }\n        if (!doc) {\n          err = createError(MISSING_DOC, 'missing');\n          return finish();\n        }\n        finish();\n      };\n    };\n  };\n\n  api._getAttachment = function (docId, attachId, attachment, opts, callback) {\n    var txn;\n    if (opts.ctx) {\n      txn = opts.ctx;\n    } else {\n      var txnResult = openTransactionSafely(idb,\n        [DOC_STORE, BY_SEQ_STORE, ATTACH_STORE], 'readonly');\n      if (txnResult.error) {\n        return callback(txnResult.error);\n      }\n      txn = txnResult.txn;\n    }\n    var digest = attachment.digest;\n    var type = attachment.content_type;\n\n    txn.objectStore(ATTACH_STORE).get(digest).onsuccess = function (e) {\n      var body = e.target.result.body;\n      readBlobData(body, type, opts.binary, function (blobData) {\n        callback(null, blobData);\n      });\n    };\n  };\n\n  api._info = function idb_info(callback) {\n    var updateSeq;\n    var docCount;\n\n    var txnResult = openTransactionSafely(idb, [META_STORE, BY_SEQ_STORE], 'readonly');\n    if (txnResult.error) {\n      return callback(txnResult.error);\n    }\n    var txn = txnResult.txn;\n    txn.objectStore(META_STORE).get(META_STORE).onsuccess = function (e) {\n      docCount = e.target.result.docCount;\n    };\n    txn.objectStore(BY_SEQ_STORE).openCursor(null, 'prev').onsuccess = function (e) {\n      var cursor = e.target.result;\n      updateSeq = cursor ? cursor.key : 0;\n    };\n\n    txn.oncomplete = function () {\n      callback(null, {\n        doc_count: docCount,\n        update_seq: updateSeq,\n        // for debugging\n        idb_attachment_format: (api._meta.blobSupport ? 'binary' : 'base64')\n      });\n    };\n  };\n\n  api._allDocs = function idb_allDocs(opts, callback) {\n    idbAllDocs(opts, idb, callback);\n  };\n\n  api._changes = function idbChanges(opts) {\n    return changes(opts, api, dbName, idb);\n  };\n\n  api._close = function (callback) {\n    // https://developer.mozilla.org/en-US/docs/IndexedDB/IDBDatabase#close\n    // \"Returns immediately and closes the connection in a separate thread...\"\n    idb.close();\n    cachedDBs.delete(dbName);\n    callback();\n  };\n\n  api._getRevisionTree = function (docId, callback) {\n    var txnResult = openTransactionSafely(idb, [DOC_STORE], 'readonly');\n    if (txnResult.error) {\n      return callback(txnResult.error);\n    }\n    var txn = txnResult.txn;\n    var req = txn.objectStore(DOC_STORE).get(docId);\n    req.onsuccess = function (event) {\n      var doc = decodeMetadata(event.target.result);\n      if (!doc) {\n        callback(createError(MISSING_DOC));\n      } else {\n        callback(null, doc.rev_tree);\n      }\n    };\n  };\n\n  // This function removes revisions of document docId\n  // which are listed in revs and sets this document\n  // revision to to rev_tree\n  api._doCompaction = function (docId, revs, callback) {\n    var stores = [\n      DOC_STORE,\n      BY_SEQ_STORE,\n      ATTACH_STORE,\n      ATTACH_AND_SEQ_STORE\n    ];\n    var txnResult = openTransactionSafely(idb, stores, 'readwrite');\n    if (txnResult.error) {\n      return callback(txnResult.error);\n    }\n    var txn = txnResult.txn;\n\n    var docStore = txn.objectStore(DOC_STORE);\n\n    docStore.get(docId).onsuccess = function (event) {\n      var metadata = decodeMetadata(event.target.result);\n      traverseRevTree(metadata.rev_tree, function (isLeaf, pos,\n                                                         revHash, ctx, opts) {\n        var rev = pos + '-' + revHash;\n        if (revs.indexOf(rev) !== -1) {\n          opts.status = 'missing';\n        }\n      });\n      compactRevs(revs, docId, txn);\n      var winningRev$$1 = metadata.winningRev;\n      var deleted = metadata.deleted;\n      txn.objectStore(DOC_STORE).put(\n        encodeMetadata(metadata, winningRev$$1, deleted));\n    };\n    txn.onabort = idbError(callback);\n    txn.oncomplete = function () {\n      callback();\n    };\n  };\n\n\n  api._getLocal = function (id, callback) {\n    var txnResult = openTransactionSafely(idb, [LOCAL_STORE], 'readonly');\n    if (txnResult.error) {\n      return callback(txnResult.error);\n    }\n    var tx = txnResult.txn;\n    var req = tx.objectStore(LOCAL_STORE).get(id);\n\n    req.onerror = idbError(callback);\n    req.onsuccess = function (e) {\n      var doc = e.target.result;\n      if (!doc) {\n        callback(createError(MISSING_DOC));\n      } else {\n        delete doc['_doc_id_rev']; // for backwards compat\n        callback(null, doc);\n      }\n    };\n  };\n\n  api._putLocal = function (doc, opts, callback) {\n    if (typeof opts === 'function') {\n      callback = opts;\n      opts = {};\n    }\n    delete doc._revisions; // ignore this, trust the rev\n    var oldRev = doc._rev;\n    var id = doc._id;\n    if (!oldRev) {\n      doc._rev = '0-1';\n    } else {\n      doc._rev = '0-' + (parseInt(oldRev.split('-')[1], 10) + 1);\n    }\n\n    var tx = opts.ctx;\n    var ret;\n    if (!tx) {\n      var txnResult = openTransactionSafely(idb, [LOCAL_STORE], 'readwrite');\n      if (txnResult.error) {\n        return callback(txnResult.error);\n      }\n      tx = txnResult.txn;\n      tx.onerror = idbError(callback);\n      tx.oncomplete = function () {\n        if (ret) {\n          callback(null, ret);\n        }\n      };\n    }\n\n    var oStore = tx.objectStore(LOCAL_STORE);\n    var req;\n    if (oldRev) {\n      req = oStore.get(id);\n      req.onsuccess = function (e) {\n        var oldDoc = e.target.result;\n        if (!oldDoc || oldDoc._rev !== oldRev) {\n          callback(createError(REV_CONFLICT));\n        } else { // update\n          var req = oStore.put(doc);\n          req.onsuccess = function () {\n            ret = {ok: true, id: doc._id, rev: doc._rev};\n            if (opts.ctx) { // return immediately\n              callback(null, ret);\n            }\n          };\n        }\n      };\n    } else { // new doc\n      req = oStore.add(doc);\n      req.onerror = function (e) {\n        // constraint error, already exists\n        callback(createError(REV_CONFLICT));\n        e.preventDefault(); // avoid transaction abort\n        e.stopPropagation(); // avoid transaction onerror\n      };\n      req.onsuccess = function () {\n        ret = {ok: true, id: doc._id, rev: doc._rev};\n        if (opts.ctx) { // return immediately\n          callback(null, ret);\n        }\n      };\n    }\n  };\n\n  api._removeLocal = function (doc, opts, callback) {\n    if (typeof opts === 'function') {\n      callback = opts;\n      opts = {};\n    }\n    var tx = opts.ctx;\n    if (!tx) {\n      var txnResult = openTransactionSafely(idb, [LOCAL_STORE], 'readwrite');\n      if (txnResult.error) {\n        return callback(txnResult.error);\n      }\n      tx = txnResult.txn;\n      tx.oncomplete = function () {\n        if (ret) {\n          callback(null, ret);\n        }\n      };\n    }\n    var ret;\n    var id = doc._id;\n    var oStore = tx.objectStore(LOCAL_STORE);\n    var req = oStore.get(id);\n\n    req.onerror = idbError(callback);\n    req.onsuccess = function (e) {\n      var oldDoc = e.target.result;\n      if (!oldDoc || oldDoc._rev !== doc._rev) {\n        callback(createError(MISSING_DOC));\n      } else {\n        oStore.delete(id);\n        ret = {ok: true, id: id, rev: '0-0'};\n        if (opts.ctx) { // return immediately\n          callback(null, ret);\n        }\n      }\n    };\n  };\n\n  api._destroy = function (opts, callback) {\n    changesHandler$1.removeAllListeners(dbName);\n\n    //Close open request for \"dbName\" database to fix ie delay.\n    var openReq = openReqList.get(dbName);\n    if (openReq && openReq.result) {\n      openReq.result.close();\n      cachedDBs.delete(dbName);\n    }\n    var req = indexedDB.deleteDatabase(dbName);\n\n    req.onsuccess = function () {\n      //Remove open request from the list.\n      openReqList.delete(dbName);\n      if (hasLocalStorage() && (dbName in localStorage)) {\n        delete localStorage[dbName];\n      }\n      callback(null, { 'ok': true });\n    };\n\n    req.onerror = idbError(callback);\n  };\n\n  var cached = cachedDBs.get(dbName);\n\n  if (cached) {\n    idb = cached.idb;\n    api._meta = cached.global;\n    return nextTick(function () {\n      callback(null, api);\n    });\n  }\n\n  var req = indexedDB.open(dbName, ADAPTER_VERSION);\n  openReqList.set(dbName, req);\n\n  req.onupgradeneeded = function (e) {\n    var db = e.target.result;\n    if (e.oldVersion < 1) {\n      return createSchema(db); // new db, initial schema\n    }\n    // do migrations\n\n    var txn = e.currentTarget.transaction;\n    // these migrations have to be done in this function, before\n    // control is returned to the event loop, because IndexedDB\n\n    if (e.oldVersion < 3) {\n      createLocalStoreSchema(db); // v2 -> v3\n    }\n    if (e.oldVersion < 4) {\n      addAttachAndSeqStore(db); // v3 -> v4\n    }\n\n    var migrations = [\n      addDeletedOrLocalIndex, // v1 -> v2\n      migrateLocalStore,      // v2 -> v3\n      migrateAttsAndSeqs,     // v3 -> v4\n      migrateMetadata         // v4 -> v5\n    ];\n\n    var i = e.oldVersion;\n\n    function next() {\n      var migration = migrations[i - 1];\n      i++;\n      if (migration) {\n        migration(txn, next);\n      }\n    }\n\n    next();\n  };\n\n  req.onsuccess = function (e) {\n\n    idb = e.target.result;\n\n    idb.onversionchange = function () {\n      idb.close();\n      cachedDBs.delete(dbName);\n    };\n\n    idb.onabort = function (e) {\n      guardedConsole('error', 'Database has a global failure', e.target.error);\n      idb.close();\n      cachedDBs.delete(dbName);\n    };\n\n    // Do a few setup operations (in parallel as much as possible):\n    // 1. Fetch meta doc\n    // 2. Check blob support\n    // 3. Calculate docCount\n    // 4. Generate an instanceId if necessary\n    // 5. Store docCount and instanceId on meta doc\n\n    var txn = idb.transaction([\n      META_STORE,\n      DETECT_BLOB_SUPPORT_STORE,\n      DOC_STORE\n    ], 'readwrite');\n\n    var storedMetaDoc = false;\n    var metaDoc;\n    var docCount;\n    var blobSupport;\n    var instanceId;\n\n    function completeSetup() {\n      if (typeof blobSupport === 'undefined' || !storedMetaDoc) {\n        return;\n      }\n      api._meta = {\n        name: dbName,\n        instanceId: instanceId,\n        blobSupport: blobSupport\n      };\n\n      cachedDBs.set(dbName, {\n        idb: idb,\n        global: api._meta\n      });\n      callback(null, api);\n    }\n\n    function storeMetaDocIfReady() {\n      if (typeof docCount === 'undefined' || typeof metaDoc === 'undefined') {\n        return;\n      }\n      var instanceKey = dbName + '_id';\n      if (instanceKey in metaDoc) {\n        instanceId = metaDoc[instanceKey];\n      } else {\n        metaDoc[instanceKey] = instanceId = uuid();\n      }\n      metaDoc.docCount = docCount;\n      txn.objectStore(META_STORE).put(metaDoc);\n    }\n\n    //\n    // fetch or generate the instanceId\n    //\n    txn.objectStore(META_STORE).get(META_STORE).onsuccess = function (e) {\n      metaDoc = e.target.result || { id: META_STORE };\n      storeMetaDocIfReady();\n    };\n\n    //\n    // countDocs\n    //\n    countDocs(txn, function (count) {\n      docCount = count;\n      storeMetaDocIfReady();\n    });\n\n    //\n    // check blob support\n    //\n    if (!blobSupportPromise) {\n      // make sure blob support is only checked once\n      blobSupportPromise = checkBlobSupport(txn);\n    }\n\n    blobSupportPromise.then(function (val) {\n      blobSupport = val;\n      completeSetup();\n    });\n\n    // only when the metadata put transaction has completed,\n    // consider the setup done\n    txn.oncomplete = function () {\n      storedMetaDoc = true;\n      completeSetup();\n    };\n    txn.onabort = idbError(callback);\n  };\n\n  req.onerror = function (e) {\n    var msg = e.target.error && e.target.error.message;\n\n    if (!msg) {\n      msg = 'Failed to open indexedDB, are you in private browsing mode?';\n    } else if (msg.indexOf(\"stored database is a higher version\") !== -1) {\n      msg = new Error('This DB was created with the newer \"indexeddb\" adapter, but you are trying to open it with the older \"idb\" adapter');\n    }\n\n    guardedConsole('error', msg);\n    callback(createError(IDB_ERROR, msg));\n  };\n}\n\nIdbPouch.valid = function () {\n  // Following #7085 buggy idb versions (typically Safari < 10.1) are\n  // considered valid.\n\n  // On Firefox SecurityError is thrown while referencing indexedDB if cookies\n  // are not allowed. `typeof indexedDB` also triggers the error.\n  try {\n    // some outdated implementations of IDB that appear on Samsung\n    // and HTC Android devices <4.4 are missing IDBKeyRange\n    return typeof indexedDB !== 'undefined' && typeof IDBKeyRange !== 'undefined';\n  } catch (e) {\n    return false;\n  }\n};\n\nfunction index (PouchDB) {\n  PouchDB.adapter('idb', IdbPouch, true);\n}\n\nexport default index;\n","import { assign, uuid, rev, invalidIdError, normalizeDdocFunctionName, parseDdocFunctionName } from 'pouchdb-utils';\nexport { invalidIdError, normalizeDdocFunctionName, parseDdocFunctionName } from 'pouchdb-utils';\nimport { atob, btoa, binaryStringToBlobOrBuffer, blobOrBufferToBinaryString, blobOrBufferToBase64 } from 'pouchdb-binary-utils';\nimport { binaryMd5 } from 'pouchdb-md5';\nimport { Map } from 'pouchdb-collections';\nimport { DOC_VALIDATION, INVALID_REV, createError, BAD_ARG, REV_CONFLICT, MISSING_DOC } from 'pouchdb-errors';\nimport { isDeleted, merge, winningRev, revExists, isLocalId } from 'pouchdb-merge';\nexport { isDeleted, isLocalId } from 'pouchdb-merge';\n\nfunction allDocsKeysQuery(api, opts) {\n  var keys = opts.keys;\n  var finalResults = {\n    offset: opts.skip\n  };\n  return Promise.all(keys.map(function (key) {\n    var subOpts = assign({key: key, deleted: 'ok'}, opts);\n    ['limit', 'skip', 'keys'].forEach(function (optKey) {\n      delete subOpts[optKey];\n    });\n    return new Promise(function (resolve, reject) {\n      api._allDocs(subOpts, function (err, res) {\n        /* istanbul ignore if */\n        if (err) {\n          return reject(err);\n        }\n        /* istanbul ignore if */\n        if (opts.update_seq && res.update_seq !== undefined) {\n          finalResults.update_seq = res.update_seq;\n        }\n        finalResults.total_rows = res.total_rows;\n        resolve(res.rows[0] || {key: key, error: 'not_found'});\n      });\n    });\n  })).then(function (results) {\n    finalResults.rows = results;\n    return finalResults;\n  });\n}\n\nfunction toObject(array) {\n  return array.reduce(function (obj, item) {\n    obj[item] = true;\n    return obj;\n  }, {});\n}\n// List of top level reserved words for doc\nvar reservedWords = toObject([\n  '_id',\n  '_rev',\n  '_attachments',\n  '_deleted',\n  '_revisions',\n  '_revs_info',\n  '_conflicts',\n  '_deleted_conflicts',\n  '_local_seq',\n  '_rev_tree',\n  //replication documents\n  '_replication_id',\n  '_replication_state',\n  '_replication_state_time',\n  '_replication_state_reason',\n  '_replication_stats',\n  // Specific to Couchbase Sync Gateway\n  '_removed'\n]);\n\n// List of reserved words that should end up the document\nvar dataWords = toObject([\n  '_attachments',\n  //replication documents\n  '_replication_id',\n  '_replication_state',\n  '_replication_state_time',\n  '_replication_state_reason',\n  '_replication_stats'\n]);\n\nfunction parseRevisionInfo(rev$$1) {\n  if (!/^\\d+-/.test(rev$$1)) {\n    return createError(INVALID_REV);\n  }\n  var idx = rev$$1.indexOf('-');\n  var left = rev$$1.substring(0, idx);\n  var right = rev$$1.substring(idx + 1);\n  return {\n    prefix: parseInt(left, 10),\n    id: right\n  };\n}\n\nfunction makeRevTreeFromRevisions(revisions, opts) {\n  var pos = revisions.start - revisions.ids.length + 1;\n\n  var revisionIds = revisions.ids;\n  var ids = [revisionIds[0], opts, []];\n\n  for (var i = 1, len = revisionIds.length; i < len; i++) {\n    ids = [revisionIds[i], {status: 'missing'}, [ids]];\n  }\n\n  return [{\n    pos: pos,\n    ids: ids\n  }];\n}\n\n// Preprocess documents, parse their revisions, assign an id and a\n// revision for new writes that are missing them, etc\nfunction parseDoc(doc, newEdits, dbOpts) {\n  if (!dbOpts) {\n    dbOpts = {\n      deterministic_revs: true\n    };\n  }\n\n  var nRevNum;\n  var newRevId;\n  var revInfo;\n  var opts = {status: 'available'};\n  if (doc._deleted) {\n    opts.deleted = true;\n  }\n\n  if (newEdits) {\n    if (!doc._id) {\n      doc._id = uuid();\n    }\n    newRevId = rev(doc, dbOpts.deterministic_revs);\n    if (doc._rev) {\n      revInfo = parseRevisionInfo(doc._rev);\n      if (revInfo.error) {\n        return revInfo;\n      }\n      doc._rev_tree = [{\n        pos: revInfo.prefix,\n        ids: [revInfo.id, {status: 'missing'}, [[newRevId, opts, []]]]\n      }];\n      nRevNum = revInfo.prefix + 1;\n    } else {\n      doc._rev_tree = [{\n        pos: 1,\n        ids : [newRevId, opts, []]\n      }];\n      nRevNum = 1;\n    }\n  } else {\n    if (doc._revisions) {\n      doc._rev_tree = makeRevTreeFromRevisions(doc._revisions, opts);\n      nRevNum = doc._revisions.start;\n      newRevId = doc._revisions.ids[0];\n    }\n    if (!doc._rev_tree) {\n      revInfo = parseRevisionInfo(doc._rev);\n      if (revInfo.error) {\n        return revInfo;\n      }\n      nRevNum = revInfo.prefix;\n      newRevId = revInfo.id;\n      doc._rev_tree = [{\n        pos: nRevNum,\n        ids: [newRevId, opts, []]\n      }];\n    }\n  }\n\n  invalidIdError(doc._id);\n\n  doc._rev = nRevNum + '-' + newRevId;\n\n  var result = {metadata : {}, data : {}};\n  for (var key in doc) {\n    /* istanbul ignore else */\n    if (Object.prototype.hasOwnProperty.call(doc, key)) {\n      var specialKey = key[0] === '_';\n      if (specialKey && !reservedWords[key]) {\n        var error = createError(DOC_VALIDATION, key);\n        error.message = DOC_VALIDATION.message + ': ' + key;\n        throw error;\n      } else if (specialKey && !dataWords[key]) {\n        result.metadata[key.slice(1)] = doc[key];\n      } else {\n        result.data[key] = doc[key];\n      }\n    }\n  }\n  return result;\n}\n\nfunction parseBase64(data) {\n  try {\n    return atob(data);\n  } catch (e) {\n    var err = createError(BAD_ARG,\n      'Attachment is not a valid base64 string');\n    return {error: err};\n  }\n}\n\nfunction preprocessString(att, blobType, callback) {\n  var asBinary = parseBase64(att.data);\n  if (asBinary.error) {\n    return callback(asBinary.error);\n  }\n\n  att.length = asBinary.length;\n  if (blobType === 'blob') {\n    att.data = binaryStringToBlobOrBuffer(asBinary, att.content_type);\n  } else if (blobType === 'base64') {\n    att.data = btoa(asBinary);\n  } else { // binary\n    att.data = asBinary;\n  }\n  binaryMd5(asBinary, function (result) {\n    att.digest = 'md5-' + result;\n    callback();\n  });\n}\n\nfunction preprocessBlob(att, blobType, callback) {\n  binaryMd5(att.data, function (md5) {\n    att.digest = 'md5-' + md5;\n    // size is for blobs (browser), length is for buffers (node)\n    att.length = att.data.size || att.data.length || 0;\n    if (blobType === 'binary') {\n      blobOrBufferToBinaryString(att.data, function (binString) {\n        att.data = binString;\n        callback();\n      });\n    } else if (blobType === 'base64') {\n      blobOrBufferToBase64(att.data, function (b64) {\n        att.data = b64;\n        callback();\n      });\n    } else {\n      callback();\n    }\n  });\n}\n\nfunction preprocessAttachment(att, blobType, callback) {\n  if (att.stub) {\n    return callback();\n  }\n  if (typeof att.data === 'string') { // input is a base64 string\n    preprocessString(att, blobType, callback);\n  } else { // input is a blob\n    preprocessBlob(att, blobType, callback);\n  }\n}\n\nfunction preprocessAttachments(docInfos, blobType, callback) {\n\n  if (!docInfos.length) {\n    return callback();\n  }\n\n  var docv = 0;\n  var overallErr;\n\n  docInfos.forEach(function (docInfo) {\n    var attachments = docInfo.data && docInfo.data._attachments ?\n      Object.keys(docInfo.data._attachments) : [];\n    var recv = 0;\n\n    if (!attachments.length) {\n      return done();\n    }\n\n    function processedAttachment(err) {\n      overallErr = err;\n      recv++;\n      if (recv === attachments.length) {\n        done();\n      }\n    }\n\n    for (var key in docInfo.data._attachments) {\n      if (docInfo.data._attachments.hasOwnProperty(key)) {\n        preprocessAttachment(docInfo.data._attachments[key],\n          blobType, processedAttachment);\n      }\n    }\n  });\n\n  function done() {\n    docv++;\n    if (docInfos.length === docv) {\n      if (overallErr) {\n        callback(overallErr);\n      } else {\n        callback();\n      }\n    }\n  }\n}\n\nfunction updateDoc(revLimit, prev, docInfo, results,\n                   i, cb, writeDoc, newEdits) {\n\n  if (revExists(prev.rev_tree, docInfo.metadata.rev) && !newEdits) {\n    results[i] = docInfo;\n    return cb();\n  }\n\n  // sometimes this is pre-calculated. historically not always\n  var previousWinningRev = prev.winningRev || winningRev(prev);\n  var previouslyDeleted = 'deleted' in prev ? prev.deleted :\n    isDeleted(prev, previousWinningRev);\n  var deleted = 'deleted' in docInfo.metadata ? docInfo.metadata.deleted :\n    isDeleted(docInfo.metadata);\n  var isRoot = /^1-/.test(docInfo.metadata.rev);\n\n  if (previouslyDeleted && !deleted && newEdits && isRoot) {\n    var newDoc = docInfo.data;\n    newDoc._rev = previousWinningRev;\n    newDoc._id = docInfo.metadata.id;\n    docInfo = parseDoc(newDoc, newEdits);\n  }\n\n  var merged = merge(prev.rev_tree, docInfo.metadata.rev_tree[0], revLimit);\n\n  var inConflict = newEdits && ((\n    (previouslyDeleted && deleted && merged.conflicts !== 'new_leaf') ||\n    (!previouslyDeleted && merged.conflicts !== 'new_leaf') ||\n    (previouslyDeleted && !deleted && merged.conflicts === 'new_branch')));\n\n  if (inConflict) {\n    var err = createError(REV_CONFLICT);\n    results[i] = err;\n    return cb();\n  }\n\n  var newRev = docInfo.metadata.rev;\n  docInfo.metadata.rev_tree = merged.tree;\n  docInfo.stemmedRevs = merged.stemmedRevs || [];\n  /* istanbul ignore else */\n  if (prev.rev_map) {\n    docInfo.metadata.rev_map = prev.rev_map; // used only by leveldb\n  }\n\n  // recalculate\n  var winningRev$$1 = winningRev(docInfo.metadata);\n  var winningRevIsDeleted = isDeleted(docInfo.metadata, winningRev$$1);\n\n  // calculate the total number of documents that were added/removed,\n  // from the perspective of total_rows/doc_count\n  var delta = (previouslyDeleted === winningRevIsDeleted) ? 0 :\n    previouslyDeleted < winningRevIsDeleted ? -1 : 1;\n\n  var newRevIsDeleted;\n  if (newRev === winningRev$$1) {\n    // if the new rev is the same as the winning rev, we can reuse that value\n    newRevIsDeleted = winningRevIsDeleted;\n  } else {\n    // if they're not the same, then we need to recalculate\n    newRevIsDeleted = isDeleted(docInfo.metadata, newRev);\n  }\n\n  writeDoc(docInfo, winningRev$$1, winningRevIsDeleted, newRevIsDeleted,\n    true, delta, i, cb);\n}\n\nfunction rootIsMissing(docInfo) {\n  return docInfo.metadata.rev_tree[0].ids[1].status === 'missing';\n}\n\nfunction processDocs(revLimit, docInfos, api, fetchedDocs, tx, results,\n                     writeDoc, opts, overallCallback) {\n\n  // Default to 1000 locally\n  revLimit = revLimit || 1000;\n\n  function insertDoc(docInfo, resultsIdx, callback) {\n    // Cant insert new deleted documents\n    var winningRev$$1 = winningRev(docInfo.metadata);\n    var deleted = isDeleted(docInfo.metadata, winningRev$$1);\n    if ('was_delete' in opts && deleted) {\n      results[resultsIdx] = createError(MISSING_DOC, 'deleted');\n      return callback();\n    }\n\n    // 4712 - detect whether a new document was inserted with a _rev\n    var inConflict = newEdits && rootIsMissing(docInfo);\n\n    if (inConflict) {\n      var err = createError(REV_CONFLICT);\n      results[resultsIdx] = err;\n      return callback();\n    }\n\n    var delta = deleted ? 0 : 1;\n\n    writeDoc(docInfo, winningRev$$1, deleted, deleted, false,\n      delta, resultsIdx, callback);\n  }\n\n  var newEdits = opts.new_edits;\n  var idsToDocs = new Map();\n\n  var docsDone = 0;\n  var docsToDo = docInfos.length;\n\n  function checkAllDocsDone() {\n    if (++docsDone === docsToDo && overallCallback) {\n      overallCallback();\n    }\n  }\n\n  docInfos.forEach(function (currentDoc, resultsIdx) {\n\n    if (currentDoc._id && isLocalId(currentDoc._id)) {\n      var fun = currentDoc._deleted ? '_removeLocal' : '_putLocal';\n      api[fun](currentDoc, {ctx: tx}, function (err, res) {\n        results[resultsIdx] = err || res;\n        checkAllDocsDone();\n      });\n      return;\n    }\n\n    var id = currentDoc.metadata.id;\n    if (idsToDocs.has(id)) {\n      docsToDo--; // duplicate\n      idsToDocs.get(id).push([currentDoc, resultsIdx]);\n    } else {\n      idsToDocs.set(id, [[currentDoc, resultsIdx]]);\n    }\n  });\n\n  // in the case of new_edits, the user can provide multiple docs\n  // with the same id. these need to be processed sequentially\n  idsToDocs.forEach(function (docs, id) {\n    var numDone = 0;\n\n    function docWritten() {\n      if (++numDone < docs.length) {\n        nextDoc();\n      } else {\n        checkAllDocsDone();\n      }\n    }\n    function nextDoc() {\n      var value = docs[numDone];\n      var currentDoc = value[0];\n      var resultsIdx = value[1];\n\n      if (fetchedDocs.has(id)) {\n        updateDoc(revLimit, fetchedDocs.get(id), currentDoc, results,\n          resultsIdx, docWritten, writeDoc, newEdits);\n      } else {\n        // Ensure stemming applies to new writes as well\n        var merged = merge([], currentDoc.metadata.rev_tree[0], revLimit);\n        currentDoc.metadata.rev_tree = merged.tree;\n        currentDoc.stemmedRevs = merged.stemmedRevs || [];\n        insertDoc(currentDoc, resultsIdx, docWritten);\n      }\n    }\n    nextDoc();\n  });\n}\n\nexport { allDocsKeysQuery, parseDoc, preprocessAttachments, processDocs, updateDoc };\n","import vuvuzela from 'vuvuzela';\n\nfunction safeJsonParse(str) {\n  // This try/catch guards against stack overflow errors.\n  // JSON.parse() is faster than vuvuzela.parse() but vuvuzela\n  // cannot overflow.\n  try {\n    return JSON.parse(str);\n  } catch (e) {\n    /* istanbul ignore next */\n    return vuvuzela.parse(str);\n  }\n}\n\nfunction safeJsonStringify(json) {\n  try {\n    return JSON.stringify(json);\n  } catch (e) {\n    /* istanbul ignore next */\n    return vuvuzela.stringify(json);\n  }\n}\n\nexport { safeJsonParse, safeJsonStringify };\n","'use strict';\n\n/**\n * Stringify/parse functions that don't operate\n * recursively, so they avoid call stack exceeded\n * errors.\n */\nexports.stringify = function stringify(input) {\n  var queue = [];\n  queue.push({obj: input});\n\n  var res = '';\n  var next, obj, prefix, val, i, arrayPrefix, keys, k, key, value, objPrefix;\n  while ((next = queue.pop())) {\n    obj = next.obj;\n    prefix = next.prefix || '';\n    val = next.val || '';\n    res += prefix;\n    if (val) {\n      res += val;\n    } else if (typeof obj !== 'object') {\n      res += typeof obj === 'undefined' ? null : JSON.stringify(obj);\n    } else if (obj === null) {\n      res += 'null';\n    } else if (Array.isArray(obj)) {\n      queue.push({val: ']'});\n      for (i = obj.length - 1; i >= 0; i--) {\n        arrayPrefix = i === 0 ? '' : ',';\n        queue.push({obj: obj[i], prefix: arrayPrefix});\n      }\n      queue.push({val: '['});\n    } else { // object\n      keys = [];\n      for (k in obj) {\n        if (obj.hasOwnProperty(k)) {\n          keys.push(k);\n        }\n      }\n      queue.push({val: '}'});\n      for (i = keys.length - 1; i >= 0; i--) {\n        key = keys[i];\n        value = obj[key];\n        objPrefix = (i > 0 ? ',' : '');\n        objPrefix += JSON.stringify(key) + ':';\n        queue.push({obj: value, prefix: objPrefix});\n      }\n      queue.push({val: '{'});\n    }\n  }\n  return res;\n};\n\n// Convenience function for the parse function.\n// This pop function is basically copied from\n// pouchCollate.parseIndexableString\nfunction pop(obj, stack, metaStack) {\n  var lastMetaElement = metaStack[metaStack.length - 1];\n  if (obj === lastMetaElement.element) {\n    // popping a meta-element, e.g. an object whose value is another object\n    metaStack.pop();\n    lastMetaElement = metaStack[metaStack.length - 1];\n  }\n  var element = lastMetaElement.element;\n  var lastElementIndex = lastMetaElement.index;\n  if (Array.isArray(element)) {\n    element.push(obj);\n  } else if (lastElementIndex === stack.length - 2) { // obj with key+value\n    var key = stack.pop();\n    element[key] = obj;\n  } else {\n    stack.push(obj); // obj with key only\n  }\n}\n\nexports.parse = function (str) {\n  var stack = [];\n  var metaStack = []; // stack for arrays and objects\n  var i = 0;\n  var collationIndex,parsedNum,numChar;\n  var parsedString,lastCh,numConsecutiveSlashes,ch;\n  var arrayElement, objElement;\n  while (true) {\n    collationIndex = str[i++];\n    if (collationIndex === '}' ||\n        collationIndex === ']' ||\n        typeof collationIndex === 'undefined') {\n      if (stack.length === 1) {\n        return stack.pop();\n      } else {\n        pop(stack.pop(), stack, metaStack);\n        continue;\n      }\n    }\n    switch (collationIndex) {\n      case ' ':\n      case '\\t':\n      case '\\n':\n      case ':':\n      case ',':\n        break;\n      case 'n':\n        i += 3; // 'ull'\n        pop(null, stack, metaStack);\n        break;\n      case 't':\n        i += 3; // 'rue'\n        pop(true, stack, metaStack);\n        break;\n      case 'f':\n        i += 4; // 'alse'\n        pop(false, stack, metaStack);\n        break;\n      case '0':\n      case '1':\n      case '2':\n      case '3':\n      case '4':\n      case '5':\n      case '6':\n      case '7':\n      case '8':\n      case '9':\n      case '-':\n        parsedNum = '';\n        i--;\n        while (true) {\n          numChar = str[i++];\n          if (/[\\d\\.\\-e\\+]/.test(numChar)) {\n            parsedNum += numChar;\n          } else {\n            i--;\n            break;\n          }\n        }\n        pop(parseFloat(parsedNum), stack, metaStack);\n        break;\n      case '\"':\n        parsedString = '';\n        lastCh = void 0;\n        numConsecutiveSlashes = 0;\n        while (true) {\n          ch = str[i++];\n          if (ch !== '\"' || (lastCh === '\\\\' &&\n              numConsecutiveSlashes % 2 === 1)) {\n            parsedString += ch;\n            lastCh = ch;\n            if (lastCh === '\\\\') {\n              numConsecutiveSlashes++;\n            } else {\n              numConsecutiveSlashes = 0;\n            }\n          } else {\n            break;\n          }\n        }\n        pop(JSON.parse('\"' + parsedString + '\"'), stack, metaStack);\n        break;\n      case '[':\n        arrayElement = { element: [], index: stack.length };\n        stack.push(arrayElement.element);\n        metaStack.push(arrayElement);\n        break;\n      case '{':\n        objElement = { element: {}, index: stack.length };\n        stack.push(objElement.element);\n        metaStack.push(objElement);\n        break;\n      default:\n        throw new Error(\n          'unexpectedly reached end of input: ' + collationIndex);\n    }\n  }\n};\n"]}